{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7484cc-6b31-4d27-b17b-b3a89569d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42, workers=True)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "\n",
    "import dcor\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import os \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50359073-4520-42d2-bcc6-63ce3e3ca145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c665f-c218-4553-96f8-80fd29f1114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 0.8937894381631173\n",
      "1 1 2 0.9036315078012284\n",
      "1 1 3 0.8955612450028319\n",
      "1 1 4 0.9064053803338077\n",
      "1 1 5 0.9113781862474227\n",
      "1 1 6 0.921937377925426\n",
      "1 1 7 0.9272431702470322\n",
      "1 1 8 0.9069060427255553\n",
      "1 1 9 0.9210962330308234\n",
      "1 1 10 0.9158083673389549\n",
      "1 1 11 0.9144444587718185\n",
      "1 1 12 0.9117958457468278\n",
      "1 1 13 0.9239614892076943\n",
      "1 1 14 0.9195901994408147\n",
      "1 1 15 0.8663313709806211\n",
      "1 1 16 0.9084815035512226\n",
      "1 1 17 0.8959779166008492\n",
      "1 1 18 0.829715891535429\n",
      "1 1 19 0.8960279537802218\n",
      "1 1 20 0.9178370311547425\n",
      "1 1 21 0.9165593342332676\n",
      "1 1 22 0.9139318612922469\n",
      "1 1 23 0.9160673577262444\n",
      "1 1 24 0.9172767488851804\n",
      "1 1 25 0.8937086219656901\n",
      "1 1 26 0.9095461185460672\n",
      "1 1 27 0.9098764557024663\n",
      "1 1 28 0.9051245286747874\n",
      "1 1 29 0.8747219468840618\n",
      "1 1 30 0.9156128441807695\n",
      "1 1 31 0.9202364505978683\n",
      "1 1 32 0.9054521643843559\n",
      "1 1 33 0.912718416079912\n",
      "1 1 34 0.9104584424286573\n",
      "1 1 35 0.9140226075459078\n",
      "1 1 36 0.9050946241871375\n",
      "1 1 37 0.9064880625534688\n",
      "1 1 38 0.9089278171511105\n",
      "1 1 39 0.9186613632279875\n",
      "1 1 40 0.9121687596755338\n",
      "1 1 41 0.912368027525146\n",
      "1 1 42 0.9142616835572842\n",
      "1 1 43 0.9013680238801258\n",
      "1 1 44 0.9074541588735242\n",
      "1 1 45 0.9029065162184259\n",
      "1 1 46 0.9126145478365577\n",
      "1 1 47 0.9094121845238182\n",
      "1 1 48 0.9241225664119808\n",
      "1 1 49 0.900987165368545\n",
      "1 1 50 0.9044817461936255\n",
      "1 1 51 0.9180276104169421\n",
      "1 1 52 0.9230681873264328\n",
      "1 1 53 0.9234769228547784\n",
      "1 1 54 0.913098098719586\n",
      "1 1 55 0.9027090434887926\n",
      "1 1 56 0.9083518651582645\n",
      "1 1 57 0.9240256985476838\n",
      "1 1 58 0.9024362088573862\n",
      "1 1 59 0.9154012340412591\n",
      "1 1 60 0.9158584288332788\n",
      "1 1 61 0.9180859265613903\n",
      "1 1 62 0.9242898195537287\n",
      "1 1 63 0.9265672042155022\n",
      "1 1 64 0.9004360104910134\n",
      "1 1 65 0.9205306573239479\n",
      "1 1 66 0.9100667652848804\n",
      "1 1 67 0.9191333985412601\n",
      "1 1 68 0.9057112912074249\n",
      "1 1 69 0.9177395141541966\n",
      "1 1 70 0.9063340851123128\n",
      "1 1 71 0.9057034394008715\n",
      "1 1 72 0.9093144137938406\n",
      "1 1 73 0.9162035200910895\n",
      "1 1 74 0.9101101747409914\n",
      "1 1 75 0.9169603448667848\n",
      "1 1 76 0.8763712744516631\n",
      "1 1 77 0.9059138223068666\n",
      "1 1 78 0.9183938139342372\n",
      "1 1 79 0.9251747551282585\n",
      "1 1 80 0.9118329265466544\n",
      "1 1 81 0.9154003610584894\n",
      "1 1 82 0.908272775787358\n",
      "1 1 83 0.9109422872611753\n",
      "1 1 84 0.9183795476727331\n",
      "1 1 85 0.927633766001245\n",
      "1 1 86 0.919762001497883\n",
      "1 1 87 0.9158266032231174\n",
      "1 1 88 0.9174495637093217\n",
      "1 1 89 0.9043082999260845\n",
      "1 1 90 0.9057068206027082\n",
      "1 1 91 0.9144371794115405\n",
      "1 1 92 0.8665315040536391\n",
      "1 1 93 0.9126053097885427\n",
      "1 1 94 0.906250947905676\n",
      "1 1 95 0.8983560059297648\n",
      "1 1 96 0.9136489949768667\n",
      "1 1 97 0.8449134529631587\n",
      "1 1 98 0.8978784018675179\n",
      "1 1 99 0.9123352533459759\n",
      "1 1 100 0.9004981690770771\n",
      "1 1 0.9086701545790938 0.015170394258548475\n",
      "1 2 1 0.9455401468771444\n",
      "1 2 2 0.9402687901435731\n",
      "1 2 3 0.9352217184104608\n",
      "1 2 4 0.9427556888577227\n",
      "1 2 5 0.9499403415856801\n",
      "1 2 6 0.9537396784595061\n",
      "1 2 7 0.9432612970732926\n",
      "1 2 8 0.9479796653642056\n",
      "1 2 9 0.932482361097177\n",
      "1 2 10 0.9489032704926285\n",
      "1 2 11 0.9534943174919474\n",
      "1 2 12 0.9444696728443978\n",
      "1 2 13 0.9500770995553665\n",
      "1 2 14 0.9400404313613756\n",
      "1 2 15 0.9466441768283197\n",
      "1 2 16 0.938834748413343\n",
      "1 2 17 0.9349636643384588\n",
      "1 2 18 0.944356323143013\n",
      "1 2 19 0.9408792857666141\n",
      "1 2 20 0.9458157638887252\n",
      "1 2 21 0.9585404587274464\n",
      "1 2 22 0.9484815871488754\n",
      "1 2 23 0.9317776833267991\n",
      "1 2 24 0.9340548245914503\n",
      "1 2 25 0.9283199469699961\n",
      "1 2 26 0.9481970453459421\n",
      "1 2 27 0.9484769394406656\n",
      "1 2 28 0.9523015366357235\n",
      "1 2 29 0.956172840412762\n",
      "1 2 30 0.9440301583119745\n",
      "1 2 31 0.9465157438660554\n",
      "1 2 32 0.9593233540419527\n",
      "1 2 33 0.9352418756786876\n",
      "1 2 34 0.9345573962645402\n",
      "1 2 35 0.9412974015234328\n",
      "1 2 36 0.9376989967351917\n",
      "1 2 37 0.9342927877011279\n",
      "1 2 38 0.9425644817554006\n",
      "1 2 39 0.9488626292635155\n",
      "1 2 40 0.9301981713198606\n",
      "1 2 41 0.9424018064789654\n",
      "1 2 42 0.9402897096531828\n",
      "1 2 43 0.9475911532397772\n",
      "1 2 44 0.9447470548419011\n",
      "1 2 45 0.945528958795305\n",
      "1 2 46 0.9346676563570367\n",
      "1 2 47 0.9389467231780972\n",
      "1 2 48 0.942442948318646\n",
      "1 2 49 0.9402456022739717\n",
      "1 2 50 0.9424519583676094\n",
      "1 2 51 0.9573412947517201\n",
      "1 2 52 0.9472540588161903\n",
      "1 2 53 0.9496441835507105\n",
      "1 2 54 0.9495541866478999\n",
      "1 2 55 0.9455586288987406\n",
      "1 2 56 0.9460757804196177\n",
      "1 2 57 0.9386704807029418\n",
      "1 2 58 0.9387107944245261\n",
      "1 2 59 0.9559004995232342\n",
      "1 2 60 0.9445809814995242\n",
      "1 2 61 0.9439254390684352\n",
      "1 2 62 0.9518117526648152\n",
      "1 2 63 0.9506683194421659\n",
      "1 2 64 0.9399799106405277\n",
      "1 2 65 0.9459365320689421\n",
      "1 2 66 0.9372577385267277\n",
      "1 2 67 0.9286258493080308\n",
      "1 2 68 0.9475172225639078\n",
      "1 2 69 0.9429952508012065\n",
      "1 2 70 0.9458498799103984\n",
      "1 2 71 0.9274581062298386\n",
      "1 2 72 0.9541162724169996\n",
      "1 2 73 0.9399750604178607\n",
      "1 2 74 0.9551150243489793\n",
      "1 2 75 0.9514202952764924\n",
      "1 2 76 0.9405303287294847\n",
      "1 2 77 0.929095097550658\n",
      "1 2 78 0.9496820028353371\n",
      "1 2 79 0.9414617635977321\n",
      "1 2 80 0.9386234086416064\n",
      "1 2 81 0.9317941332216022\n",
      "1 2 82 0.9437678981231936\n",
      "1 2 83 0.9506374204516781\n",
      "1 2 84 0.947215699786501\n",
      "1 2 85 0.9387244554665513\n",
      "1 2 86 0.9502073047697702\n",
      "1 2 87 0.9358353792730291\n",
      "1 2 88 0.9388697397039638\n",
      "1 2 89 0.9436491591047594\n",
      "1 2 90 0.9381495785379419\n",
      "1 2 91 0.9477627185687463\n",
      "1 2 92 0.95170117970624\n",
      "1 2 93 0.9463291690100484\n",
      "1 2 94 0.9503296714816645\n",
      "1 2 95 0.9447265430368746\n",
      "1 2 96 0.9501870003875027\n",
      "1 2 97 0.952918487345045\n",
      "1 2 98 0.9440447413807403\n",
      "1 2 99 0.9501927510790437\n",
      "1 2 100 0.9423593714408462\n",
      "1 2 0.9439462641870984 0.007075512618632863\n",
      "1 3 1 0.8954973639411411\n",
      "1 3 2 0.8149572953135904\n",
      "1 3 3 0.9186386868299241\n",
      "1 3 4 0.8733451538684175\n",
      "1 3 5 0.8854264060058114\n",
      "1 3 6 0.9310270189138002\n",
      "1 3 7 0.8579300163729184\n",
      "1 3 8 0.8874034442423891\n",
      "1 3 9 0.8558335307719902\n",
      "1 3 10 0.905579056240101\n",
      "1 3 11 0.913101146163907\n",
      "1 3 12 0.9131169603294245\n",
      "1 3 13 0.8744043077686796\n",
      "1 3 14 0.8997430994117867\n",
      "1 3 15 0.9278749427576729\n",
      "1 3 16 0.8979637845329808\n",
      "1 3 17 0.8799716594612115\n",
      "1 3 18 0.907827686621654\n",
      "1 3 19 0.8932541662876041\n",
      "1 3 20 0.8743004885059741\n",
      "1 3 21 0.8276916676647914\n",
      "1 3 22 0.8523780000082021\n",
      "1 3 23 0.8786795461486797\n",
      "1 3 24 0.8698515522456756\n",
      "1 3 25 0.8840930653882001\n",
      "1 3 26 0.8810246523122875\n",
      "1 3 27 0.9165800851716883\n",
      "1 3 28 0.8797190846333401\n",
      "1 3 29 0.8468053310452038\n",
      "1 3 30 0.8919079406345802\n",
      "1 3 31 0.8678663724680917\n",
      "1 3 32 0.8685354324316633\n",
      "1 3 33 0.9079253882298819\n",
      "1 3 34 0.8788934915860784\n",
      "1 3 35 0.8686138826682333\n",
      "1 3 36 0.8525740602096507\n",
      "1 3 37 0.8755103815707717\n",
      "1 3 38 0.870097585566962\n",
      "1 3 39 0.9177796721765523\n",
      "1 3 40 0.8736909943985532\n",
      "1 3 41 0.9064190298185794\n",
      "1 3 42 0.8960069392677195\n",
      "1 3 43 0.8734743507315786\n",
      "1 3 44 0.8901980307658116\n",
      "1 3 45 0.8756596937139609\n",
      "1 3 46 0.8697435332977197\n",
      "1 3 47 0.8845855436324508\n",
      "1 3 48 0.8990619992856081\n",
      "1 3 49 0.8732170753870132\n",
      "1 3 50 0.8343304873949988\n",
      "1 3 51 0.8764867144158855\n",
      "1 3 52 0.8734855125159677\n",
      "1 3 53 0.882182222143244\n",
      "1 3 54 0.8085307072938409\n",
      "1 3 55 0.8998644320431382\n",
      "1 3 56 0.8946870671628193\n",
      "1 3 57 0.914776526047037\n",
      "1 3 58 0.8742424934610677\n",
      "1 3 59 0.8856655959437302\n",
      "1 3 60 0.8799264249853871\n",
      "1 3 61 0.9079621282752258\n",
      "1 3 62 0.8840950382649749\n",
      "1 3 63 0.8918109263164486\n",
      "1 3 64 0.9041381890809727\n",
      "1 3 65 0.8899324438275839\n",
      "1 3 66 0.8724670964721731\n",
      "1 3 67 0.901774265196793\n",
      "1 3 68 0.8725742648674641\n",
      "1 3 69 0.858202925635194\n",
      "1 3 70 0.8361106048575213\n",
      "1 3 71 0.8778220381464498\n",
      "1 3 72 0.8391717445350797\n",
      "1 3 73 0.8699722956954988\n",
      "1 3 74 0.8848977781767722\n",
      "1 3 75 0.8861126393938884\n",
      "1 3 76 0.8186189502482085\n",
      "1 3 77 0.8723805172811188\n",
      "1 3 78 0.9011356877146137\n",
      "1 3 79 0.8915338482581204\n",
      "1 3 80 0.860168076261813\n",
      "1 3 81 0.9036019802788668\n",
      "1 3 82 0.8886860800607177\n",
      "1 3 83 0.9050344908943292\n",
      "1 3 84 0.8714122947367565\n",
      "1 3 85 0.8445796693610028\n",
      "1 3 86 0.8521958965277064\n",
      "1 3 87 0.8758479846643685\n",
      "1 3 88 0.8619501319399004\n",
      "1 3 89 0.8848016399780564\n",
      "1 3 90 0.8847323361833497\n",
      "1 3 91 0.889757916786957\n",
      "1 3 92 0.9045364382931848\n",
      "1 3 93 0.9123054380842249\n",
      "1 3 94 0.9000107585528137\n",
      "1 3 95 0.8711517013037844\n",
      "1 3 96 0.8992202387093191\n",
      "1 3 97 0.8108892464005911\n",
      "1 3 98 0.8597274593475306\n",
      "1 3 99 0.8558846394254481\n",
      "1 3 100 0.9315506947380788\n",
      "1 3 0.8803871727498052 0.025128562424678695\n",
      "2 1 1 0.48600455567903095\n",
      "2 1 2 0.5220627835364269\n",
      "2 1 3 0.6341560062833942\n",
      "2 1 4 0.7588996217874562\n",
      "2 1 5 0.44918441122419733\n",
      "2 1 6 0.6142604294289152\n",
      "2 1 7 0.49076637212759083\n",
      "2 1 8 0.6329218104012879\n",
      "2 1 9 0.6494205796705297\n",
      "2 1 10 0.6322470704258236\n",
      "2 1 11 0.681907205549248\n",
      "2 1 12 0.7186082573619043\n",
      "2 1 13 0.5843766373695441\n",
      "2 1 14 0.7164352948494059\n",
      "2 1 15 0.7443026963516227\n",
      "2 1 16 0.5737370553219023\n",
      "2 1 17 0.6963191069454344\n",
      "2 1 18 0.6486092718894506\n",
      "2 1 19 0.5966420332490993\n",
      "2 1 20 0.7002095861734479\n",
      "2 1 21 0.7346787253932374\n",
      "2 1 22 0.7226356767204533\n",
      "2 1 23 0.4853660217536903\n",
      "2 1 24 0.5798518862312786\n",
      "2 1 25 0.627919162663906\n",
      "2 1 26 0.6396498015291954\n",
      "2 1 27 0.6976742697790983\n",
      "2 1 28 0.7651490646829777\n",
      "2 1 29 0.4463302268147069\n",
      "2 1 30 0.653040202138264\n",
      "2 1 31 0.6323723407363228\n",
      "2 1 32 0.33050959366364313\n",
      "2 1 33 0.7377843474799994\n",
      "2 1 34 0.627371257649545\n",
      "2 1 35 0.6700274820356782\n",
      "2 1 36 0.714533086222613\n",
      "2 1 37 0.7288892154410826\n",
      "2 1 38 0.7022501189395565\n",
      "2 1 39 0.6806704190729439\n",
      "2 1 40 0.8006497603636082\n",
      "2 1 41 0.6490732889885791\n",
      "2 1 42 0.7164018945669646\n",
      "2 1 43 0.7882063065140433\n",
      "2 1 44 0.6627722240291154\n",
      "2 1 45 0.7728808987054727\n",
      "2 1 46 0.5948638132342827\n",
      "2 1 47 0.7181174826746707\n",
      "2 1 48 0.6839023886145632\n",
      "2 1 49 0.7236663799431257\n",
      "2 1 50 0.6579317081954762\n",
      "2 1 51 0.6891305492802101\n",
      "2 1 52 0.7307174354299252\n",
      "2 1 53 0.6365282926127999\n",
      "2 1 54 0.6277646185590616\n",
      "2 1 55 0.547382407865179\n",
      "2 1 56 0.7160991008883242\n",
      "2 1 57 0.6879198225293588\n",
      "2 1 58 0.7141876386632434\n",
      "2 1 59 0.547763850651365\n",
      "2 1 60 0.7500506169340561\n",
      "2 1 61 0.7047094918394587\n",
      "2 1 62 0.607811127374106\n",
      "2 1 63 0.6264094118139365\n",
      "2 1 64 0.7714852921887309\n",
      "2 1 65 0.5585832663697768\n",
      "2 1 66 0.6138099216978906\n",
      "2 1 67 0.6204865022222786\n",
      "2 1 68 0.6418434337089417\n",
      "2 1 69 0.6564591718753361\n",
      "2 1 70 0.7197001441368758\n",
      "2 1 71 0.6642322254556495\n",
      "2 1 72 0.6305609278356594\n",
      "2 1 73 0.721339244708351\n",
      "2 1 74 0.6854320723850682\n",
      "2 1 75 0.648082456203771\n",
      "2 1 76 0.768702812384284\n",
      "2 1 77 0.4909936130718746\n",
      "2 1 78 0.7332903795107669\n",
      "2 1 79 0.7282303475723472\n",
      "2 1 80 0.6873677721886019\n",
      "2 1 81 0.7477053959335335\n",
      "2 1 82 0.6527106547496927\n",
      "2 1 83 0.7725103288296515\n",
      "2 1 84 0.6136499572708796\n",
      "2 1 85 0.6654960174946583\n",
      "2 1 86 0.6572316698929623\n",
      "2 1 87 0.728570489729389\n",
      "2 1 88 0.617731036728961\n",
      "2 1 89 0.6438579946933984\n",
      "2 1 90 0.6409047693672033\n",
      "2 1 91 0.5788740483739108\n",
      "2 1 92 0.7222994528224043\n",
      "2 1 93 0.34442072421592257\n",
      "2 1 94 0.6495049298482436\n",
      "2 1 95 0.5603618953298765\n",
      "2 1 96 0.6444681394722673\n",
      "2 1 97 0.716734149901333\n",
      "2 1 98 0.7182026562455075\n",
      "2 1 99 0.5918440417455973\n",
      "2 1 100 0.5513247699281091\n",
      "2 1 0.6522171690093456 0.0880632669713644\n",
      "2 2 1 0.875740975242344\n",
      "2 2 2 0.6982264045972347\n",
      "2 2 3 0.8980830184019736\n",
      "2 2 4 0.8818587150962751\n",
      "2 2 5 0.8594917788893942\n",
      "2 2 6 0.7308481469389992\n",
      "2 2 7 0.7552994130679074\n",
      "2 2 8 0.7712998301655213\n",
      "2 2 9 0.7597675558015496\n",
      "2 2 10 0.8681648976305729\n",
      "2 2 11 0.8433891780614301\n",
      "2 2 12 0.8807520731894627\n",
      "2 2 13 0.7492892300078399\n",
      "2 2 14 0.8929388395286382\n",
      "2 2 15 0.6793166405346771\n",
      "2 2 16 0.7720185321804808\n",
      "2 2 17 0.8860283930540934\n",
      "2 2 18 0.5620205741588211\n",
      "2 2 19 0.8141432001402097\n",
      "2 2 20 0.6587038030045256\n",
      "2 2 21 0.8821040253816033\n",
      "2 2 22 0.8925980078874952\n",
      "2 2 23 0.890301350696262\n",
      "2 2 24 0.9014406696162013\n",
      "2 2 25 0.8797512789234417\n",
      "2 2 26 0.8000720914380711\n",
      "2 2 27 0.7669196350715115\n",
      "2 2 28 0.7589090630509338\n",
      "2 2 29 0.8825969532278202\n",
      "2 2 30 0.8838091730854252\n",
      "2 2 31 0.879476218409891\n",
      "2 2 32 0.7932586561991793\n",
      "2 2 33 0.7805234571866733\n",
      "2 2 34 0.7350396940804296\n",
      "2 2 35 0.8934651278707251\n",
      "2 2 36 0.759065311922633\n",
      "2 2 37 0.8669802479151584\n",
      "2 2 38 0.7261292890120661\n",
      "2 2 39 0.8839875130071838\n",
      "2 2 40 0.8410378987919334\n",
      "2 2 41 0.7315439107724354\n",
      "2 2 42 0.8614256355883657\n",
      "2 2 43 0.861882754313935\n",
      "2 2 44 0.729610857691094\n",
      "2 2 45 0.8970908151171785\n",
      "2 2 46 0.8475771004788183\n",
      "2 2 47 0.8822492003793982\n",
      "2 2 48 0.8038684392549679\n",
      "2 2 49 0.7351114055704624\n",
      "2 2 50 0.7231276440500872\n",
      "2 2 51 0.7649921326866902\n",
      "2 2 52 0.8061427440245275\n",
      "2 2 53 0.5935502734088998\n",
      "2 2 54 0.7671869962548863\n",
      "2 2 55 0.8050110758005327\n",
      "2 2 56 0.7798532374731328\n",
      "2 2 57 0.8035989426515168\n",
      "2 2 58 0.8842164652573077\n",
      "2 2 59 0.8652314687995413\n",
      "2 2 60 0.8554528836387764\n",
      "2 2 61 0.9178625191799062\n",
      "2 2 62 0.7792239106895589\n",
      "2 2 63 0.8061003175561975\n",
      "2 2 64 0.8679014360014862\n",
      "2 2 65 0.8754637429772258\n",
      "2 2 66 0.7863363726960354\n",
      "2 2 67 0.7814291240214662\n",
      "2 2 68 0.7383692695726148\n",
      "2 2 69 0.7608539288513759\n",
      "2 2 70 0.8981804034326527\n",
      "2 2 71 0.7878699541855576\n",
      "2 2 72 0.8780467773361537\n",
      "2 2 73 0.8076901066630721\n",
      "2 2 74 0.8691625324984972\n",
      "2 2 75 0.7710572384795723\n",
      "2 2 76 0.7192766361706018\n",
      "2 2 77 0.8885645461510916\n",
      "2 2 78 0.6467227167673314\n",
      "2 2 79 0.896583370612289\n",
      "2 2 80 0.8947744505805927\n",
      "2 2 81 0.869974863925608\n",
      "2 2 82 0.8866423652158584\n",
      "2 2 83 0.7295815307756081\n",
      "2 2 84 0.7829868667173913\n",
      "2 2 85 0.7648946140084195\n",
      "2 2 86 0.7959939337501262\n",
      "2 2 87 0.8842565828141097\n",
      "2 2 88 0.6266949285648408\n",
      "2 2 89 0.78729603179987\n",
      "2 2 90 0.9030820650130962\n",
      "2 2 91 0.7257272208882009\n",
      "2 2 92 0.891149407136038\n",
      "2 2 93 0.8971578231708568\n",
      "2 2 94 0.8804029849976255\n",
      "2 2 95 0.783665396362244\n",
      "2 2 96 0.8682135696521438\n",
      "2 2 97 0.796692229340222\n",
      "2 2 98 0.875045357106798\n",
      "2 2 99 0.9092394150485971\n",
      "2 2 100 0.892222907826604\n",
      "2 2 0.8145596029421669 0.0765540173560004\n",
      "2 3 1 0.7919602106877286\n",
      "2 3 2 0.8748743254874137\n",
      "2 3 3 0.8590304412980799\n",
      "2 3 4 0.8620333107758702\n",
      "2 3 5 0.8423601349474662\n",
      "2 3 6 0.8793480639117961\n",
      "2 3 7 0.8453801467968203\n",
      "2 3 8 0.8729250208352826\n",
      "2 3 9 0.8995565753879676\n",
      "2 3 10 0.890939387139467\n",
      "2 3 11 0.8288802886815618\n",
      "2 3 12 0.8705649215036296\n",
      "2 3 13 0.876672155125747\n",
      "2 3 14 0.6048534534127712\n",
      "2 3 15 0.8530848257643201\n",
      "2 3 16 0.8362420584959416\n",
      "2 3 17 0.8979301081130329\n",
      "2 3 18 0.885680832746117\n",
      "2 3 19 0.8386218498312538\n",
      "2 3 20 0.8958190557877417\n",
      "2 3 21 0.8397389773360008\n",
      "2 3 22 0.8669737195108942\n",
      "2 3 23 0.8896372413099116\n",
      "2 3 24 0.8803093499059781\n",
      "2 3 25 0.8439730983725312\n",
      "2 3 26 0.8748675666211119\n",
      "2 3 27 0.8433002757294504\n",
      "2 3 28 0.8480960749510335\n",
      "2 3 29 0.8686530984098753\n",
      "2 3 30 0.8555505600039507\n",
      "2 3 31 0.8294729213133158\n",
      "2 3 32 0.8669049480916718\n",
      "2 3 33 0.818671079693259\n",
      "2 3 34 0.8724763670233453\n",
      "2 3 35 0.8640157409803529\n",
      "2 3 36 0.8294153819749003\n",
      "2 3 37 0.8787387532153715\n",
      "2 3 38 0.8779027303437681\n",
      "2 3 39 0.8821745876126088\n",
      "2 3 40 0.8583301029534004\n",
      "2 3 41 0.8747852953301988\n",
      "2 3 42 0.827444554722936\n",
      "2 3 43 0.8931333392966504\n",
      "2 3 44 0.8367391493959708\n",
      "2 3 45 0.847352933138893\n",
      "2 3 46 0.847403522343968\n",
      "2 3 47 0.8578914239210166\n",
      "2 3 48 0.8782749899046443\n",
      "2 3 49 0.8721211493464968\n",
      "2 3 50 0.89122383136602\n",
      "2 3 51 0.8743655742715128\n",
      "2 3 52 0.8393638246403308\n",
      "2 3 53 0.8323221105149421\n",
      "2 3 54 0.8184326129503048\n",
      "2 3 55 0.8789488903747706\n",
      "2 3 56 0.8527456297520654\n",
      "2 3 57 0.8564297306578713\n",
      "2 3 58 0.8868114288703167\n",
      "2 3 59 0.8832987444753236\n",
      "2 3 60 0.8524672777059401\n",
      "2 3 61 0.8524822873232949\n",
      "2 3 62 0.516327851452338\n",
      "2 3 63 0.8628625175142713\n",
      "2 3 64 0.8242751760150933\n",
      "2 3 65 0.8888776445550373\n",
      "2 3 66 0.8656190810099891\n",
      "2 3 67 0.88881683139148\n",
      "2 3 68 0.8601811032208675\n",
      "2 3 69 0.8677296700092099\n",
      "2 3 70 0.8391862278888071\n",
      "2 3 71 0.8611529297775566\n",
      "2 3 72 0.8826245203671472\n",
      "2 3 73 0.8521194197812094\n",
      "2 3 74 0.7418384500188556\n",
      "2 3 75 0.8363588251531603\n",
      "2 3 76 0.8678241509371044\n",
      "2 3 77 0.7387065494014072\n",
      "2 3 78 0.8448796843494815\n",
      "2 3 79 0.8839734953173806\n",
      "2 3 80 0.8690147899810037\n",
      "2 3 81 0.879590644812066\n",
      "2 3 82 0.9051102026482473\n",
      "2 3 83 0.877906671848588\n",
      "2 3 84 0.8683478786371658\n",
      "2 3 85 0.8467130159421281\n",
      "2 3 86 0.8340851580295139\n",
      "2 3 87 0.844151406133754\n",
      "2 3 88 0.8859747008827029\n",
      "2 3 89 0.8732347930153859\n",
      "2 3 90 0.8634373264875693\n",
      "2 3 91 0.8832941511637036\n",
      "2 3 92 0.8743469219495263\n",
      "2 3 93 0.8989331045510488\n",
      "2 3 94 0.8635012809562769\n",
      "2 3 95 0.7923073540248444\n",
      "2 3 96 0.8484416923554934\n",
      "2 3 97 0.8728202024586741\n",
      "2 3 98 0.8781580839685814\n",
      "2 3 99 0.8817418823629583\n",
      "2 3 100 0.6369405717244486\n",
      "2 3 0.8515140400448028 0.055163386271396525\n",
      "3 1 1 0.5254541846363814\n",
      "3 1 2 0.3230394090730278\n",
      "3 1 3 0.5365248560759397\n",
      "3 1 4 0.5099817076378708\n",
      "3 1 5 0.30957649189023123\n",
      "3 1 6 0.6475933572589333\n",
      "3 1 7 0.17835338212594948\n",
      "3 1 8 0.22819877191233987\n",
      "3 1 9 0.2588712541439411\n",
      "3 1 10 0.6708103024725407\n",
      "3 1 11 0.2316184568357027\n",
      "3 1 12 0.7148213917959869\n",
      "3 1 13 0.5369449153740347\n",
      "3 1 14 0.5366448418844584\n",
      "3 1 15 0.4532302291445752\n",
      "3 1 16 0.579886391493709\n",
      "3 1 17 0.15309044622416118\n",
      "3 1 18 0.645307602069892\n",
      "3 1 19 0.42249118212836945\n",
      "3 1 20 0.6028126654745154\n",
      "3 1 21 0.6593705007972397\n",
      "3 1 22 0.5596465890351887\n",
      "3 1 23 0.4482850702889423\n",
      "3 1 24 0.5306082815720078\n",
      "3 1 25 0.48870365487145023\n",
      "3 1 26 0.5349446294298488\n",
      "3 1 27 0.521109670100402\n",
      "3 1 28 0.6085261727762012\n",
      "3 1 29 0.2808667289047973\n",
      "3 1 30 0.3980710001933165\n",
      "3 1 31 0.48165592436418203\n",
      "3 1 32 0.3392556968935922\n",
      "3 1 33 0.4846282379672793\n",
      "3 1 34 0.5553441451687489\n",
      "3 1 35 0.2770964382103159\n",
      "3 1 36 0.30972628427376053\n",
      "3 1 37 0.6212503742697234\n",
      "3 1 38 0.5501628559429632\n",
      "3 1 39 0.37232021204473914\n",
      "3 1 40 0.28750793241820627\n",
      "3 1 41 0.5880708794409826\n",
      "3 1 42 0.5287891643595922\n",
      "3 1 43 0.4981588658725222\n",
      "3 1 44 0.5696339264187741\n",
      "3 1 45 0.5869641640025441\n",
      "3 1 46 0.6297171705826322\n",
      "3 1 47 0.5876998024560524\n",
      "3 1 48 0.5556746820277101\n",
      "3 1 49 0.6260512856489988\n",
      "3 1 50 0.46922497211503456\n",
      "3 1 51 0.18603145644804975\n",
      "3 1 52 0.21662662189051382\n",
      "3 1 53 0.649669416531748\n",
      "3 1 54 0.5791695198279557\n",
      "3 1 55 0.39409158659255605\n",
      "3 1 56 0.600835715782313\n",
      "3 1 57 0.5247732495886402\n",
      "3 1 58 0.5204750868061443\n",
      "3 1 59 0.5279124325099384\n",
      "3 1 60 0.5762245059373643\n",
      "3 1 61 0.5804163988220411\n",
      "3 1 62 0.39056898512568444\n",
      "3 1 63 0.5894579485382985\n",
      "3 1 64 0.6235933959142349\n",
      "3 1 65 0.46502242707407054\n",
      "3 1 66 0.5540022420433296\n",
      "3 1 67 0.5702883459178346\n",
      "3 1 68 0.30547377324741204\n",
      "3 1 69 0.5696616983687031\n",
      "3 1 70 0.6156961465900443\n",
      "3 1 71 0.45301391203632085\n",
      "3 1 72 0.6148362376780262\n",
      "3 1 73 0.5735736596495602\n",
      "3 1 74 0.2604903993541673\n",
      "3 1 75 0.4594462791432328\n",
      "3 1 76 0.555473354881137\n",
      "3 1 77 0.4573517138300742\n",
      "3 1 78 0.5152380451866789\n",
      "3 1 79 0.6004771991308891\n",
      "3 1 80 0.4896002753492858\n",
      "3 1 81 0.44536590738523957\n",
      "3 1 82 0.20743931145946473\n",
      "3 1 83 0.4744289178329117\n",
      "3 1 84 0.5329381549163016\n",
      "3 1 85 0.5419049129527038\n",
      "3 1 86 0.4625912810798113\n",
      "3 1 87 0.5680369862940418\n",
      "3 1 88 0.5854448735596686\n",
      "3 1 89 0.5567410272674042\n",
      "3 1 90 0.5493982792741993\n",
      "3 1 91 0.46124383850418094\n",
      "3 1 92 0.5558685158671637\n",
      "3 1 93 0.6316112991948793\n",
      "3 1 94 0.5223059001209499\n",
      "3 1 95 0.13911331653210027\n",
      "3 1 96 0.6249015771964095\n",
      "3 1 97 0.6526972881757067\n",
      "3 1 98 0.6417848875504565\n",
      "3 1 99 0.5556245549384213\n",
      "3 1 100 0.6403296436448174\n",
      "3 1 0.49383579655643395 0.13371446493226885\n",
      "3 2 1 0.8104701601925928\n",
      "3 2 2 0.8599976395897759\n",
      "3 2 3 0.8650202291962422\n",
      "3 2 4 0.8776222801707924\n",
      "3 2 5 0.8749659611930314\n",
      "3 2 6 0.8896106975373433\n",
      "3 2 7 0.8550269592825896\n",
      "3 2 8 0.8426220600580906\n",
      "3 2 9 0.7916005758565035\n",
      "3 2 10 0.8646145886572046\n",
      "3 2 11 0.8168781907439238\n",
      "3 2 12 0.8415120082949599\n",
      "3 2 13 0.766116972485435\n",
      "3 2 14 0.8737888029967741\n",
      "3 2 15 0.8676147875002779\n",
      "3 2 16 0.8637016753082771\n",
      "3 2 17 0.8643855701544685\n",
      "3 2 18 0.8511482954882127\n",
      "3 2 19 0.6681658057996243\n",
      "3 2 20 0.8383999813037435\n",
      "3 2 21 0.8930000686613296\n",
      "3 2 22 0.7940443306610391\n",
      "3 2 23 0.8398624697442193\n",
      "3 2 24 0.8830620197171405\n",
      "3 2 25 0.7683619941120062\n",
      "3 2 26 0.8613976915211222\n",
      "3 2 27 0.8891041011355668\n",
      "3 2 28 0.8189249836688812\n",
      "3 2 29 0.8732209278297403\n",
      "3 2 30 0.7785452682613734\n",
      "3 2 31 0.850468889886418\n",
      "3 2 32 0.8729546398754704\n",
      "3 2 33 0.7273733038893638\n",
      "3 2 34 0.8103703725873048\n",
      "3 2 35 0.8679990724411369\n",
      "3 2 36 0.8400086494925135\n",
      "3 2 37 0.8641859017525564\n",
      "3 2 38 0.7774519205947055\n",
      "3 2 39 0.870033869393547\n",
      "3 2 40 0.6906981527622659\n",
      "3 2 41 0.8384010195759339\n",
      "3 2 42 0.8763657859373473\n",
      "3 2 43 0.7387556566479875\n",
      "3 2 44 0.8782606499588063\n",
      "3 2 45 0.8566461210721444\n",
      "3 2 46 0.8684978054462045\n",
      "3 2 47 0.8683389650697483\n",
      "3 2 48 0.8289610211282421\n",
      "3 2 49 0.8450885117487983\n",
      "3 2 50 0.5348054832088258\n",
      "3 2 51 0.8006381166727919\n",
      "3 2 52 0.8739848373333772\n",
      "3 2 53 0.8328225976940116\n",
      "3 2 54 0.8664689098681123\n",
      "3 2 55 0.8087048928435084\n",
      "3 2 56 0.8511725896189329\n",
      "3 2 57 0.8605744751197871\n",
      "3 2 58 0.8570377161478663\n",
      "3 2 59 0.850773891720141\n",
      "3 2 60 0.5895053680116651\n",
      "3 2 61 0.8942245249044276\n",
      "3 2 62 0.8823518934367147\n",
      "3 2 63 0.8473913451918089\n",
      "3 2 64 0.854089091050972\n",
      "3 2 65 0.8903669160124821\n",
      "3 2 66 0.6607081973221312\n",
      "3 2 67 0.8281720125250782\n",
      "3 2 68 0.8480450806006724\n",
      "3 2 69 0.8010361959708573\n",
      "3 2 70 0.7627877451473872\n",
      "3 2 71 0.8261295970821001\n",
      "3 2 72 0.8183697168841592\n",
      "3 2 73 0.8274083973526513\n",
      "3 2 74 0.862247179740689\n",
      "3 2 75 0.8709242011070095\n",
      "3 2 76 0.698502488815966\n",
      "3 2 77 0.849403211516671\n",
      "3 2 78 0.8076650073785359\n",
      "3 2 79 0.7733715836635368\n",
      "3 2 80 0.8525465804286011\n",
      "3 2 81 0.8810278446629953\n",
      "3 2 82 0.8593841144727105\n",
      "3 2 83 0.8436528220072596\n",
      "3 2 84 0.8947102232391412\n",
      "3 2 85 0.8430719638713312\n",
      "3 2 86 0.8512345464221583\n",
      "3 2 87 0.8022087416221939\n",
      "3 2 88 0.8498252267463339\n",
      "3 2 89 0.860581603179892\n",
      "3 2 90 0.8603629144932333\n",
      "3 2 91 0.8546833034283727\n",
      "3 2 92 0.8305243977784688\n",
      "3 2 93 0.8851588322997256\n",
      "3 2 94 0.8878375022398193\n",
      "3 2 95 0.8322023773161678\n",
      "3 2 96 0.7795258645255547\n",
      "3 2 97 0.7747970554167458\n",
      "3 2 98 0.8452529427593042\n",
      "3 2 99 0.8631537079814324\n",
      "3 2 100 0.8391384282826235\n",
      "3 2 0.8307421166349971 0.06132857098876986\n",
      "3 3 1 0.855639262376945\n",
      "3 3 2 0.8282892084921395\n",
      "3 3 3 0.842583210226563\n",
      "3 3 4 0.7643194063388091\n",
      "3 3 5 0.856695000297582\n",
      "3 3 6 0.8393846474496851\n",
      "3 3 7 0.8279771616079328\n",
      "3 3 8 0.7561890321067978\n",
      "3 3 9 0.7964890481867302\n",
      "3 3 10 0.5328049677164998\n",
      "3 3 11 0.8159145593059619\n",
      "3 3 12 0.8128046296963943\n",
      "3 3 13 0.8064070853967694\n",
      "3 3 14 0.8615105171093689\n",
      "3 3 15 0.8379083400791486\n",
      "3 3 16 0.8316876784708211\n",
      "3 3 17 0.8217256347871821\n",
      "3 3 18 0.839518903623233\n",
      "3 3 19 0.8120256834951324\n",
      "3 3 20 0.8745529885265731\n",
      "3 3 21 0.806672967528963\n",
      "3 3 22 0.8683515494226864\n",
      "3 3 23 0.8201904094912476\n",
      "3 3 24 0.8404967905964992\n",
      "3 3 25 0.7635718132817609\n",
      "3 3 26 0.7697473271288805\n",
      "3 3 27 0.7904249579366601\n",
      "3 3 28 0.5739001227661423\n",
      "3 3 29 0.8538062949880884\n",
      "3 3 30 0.8236965874758319\n",
      "3 3 31 0.6773147687990817\n",
      "3 3 32 0.8615988629576904\n",
      "3 3 33 0.7977499341770645\n",
      "3 3 34 0.8235311452055586\n",
      "3 3 35 0.6379313511366044\n",
      "3 3 36 0.7900214187215446\n",
      "3 3 37 0.8122017465792284\n",
      "3 3 38 0.7597592948404271\n",
      "3 3 39 0.7682153215881092\n",
      "3 3 40 0.8710205591382207\n",
      "3 3 41 0.7911492561431953\n",
      "3 3 42 0.8077504770218072\n",
      "3 3 43 0.7329060966662533\n",
      "3 3 44 0.7610437816484878\n",
      "3 3 45 0.7307891103606009\n",
      "3 3 46 0.6405518705217482\n",
      "3 3 47 0.8515351293414437\n",
      "3 3 48 0.8482572429501701\n",
      "3 3 49 0.8212432997425446\n",
      "3 3 50 0.8358513364334978\n",
      "3 3 51 0.8047163280208961\n",
      "3 3 52 0.8052137476073368\n",
      "3 3 53 0.8825064112769904\n",
      "3 3 54 0.8455228763226659\n",
      "3 3 55 0.8398202618368488\n",
      "3 3 56 0.8500947606600255\n",
      "3 3 57 0.8609379675817275\n",
      "3 3 58 0.7987157890132994\n",
      "3 3 59 0.8299188551443746\n",
      "3 3 60 0.7970716972669217\n",
      "3 3 61 0.7897547092731269\n",
      "3 3 62 0.8567637590119769\n",
      "3 3 63 0.7568467343872515\n",
      "3 3 64 0.81129870670671\n",
      "3 3 65 0.6939782689684028\n",
      "3 3 66 0.7228400561473015\n",
      "3 3 67 0.8332079991086961\n",
      "3 3 68 0.8236181767699328\n",
      "3 3 69 0.8116029645264132\n",
      "3 3 70 0.5637393678929752\n",
      "3 3 71 0.669020442820037\n",
      "3 3 72 0.8359722108165415\n",
      "3 3 73 0.4871232800635297\n",
      "3 3 74 0.8267095824820285\n",
      "3 3 75 0.8236714055530016\n",
      "3 3 76 0.697448338868027\n",
      "3 3 77 0.6969857971394026\n",
      "3 3 78 0.8343279352664984\n",
      "3 3 79 0.7637831859666122\n",
      "3 3 80 0.8032665562945224\n",
      "3 3 81 0.8577418304989146\n",
      "3 3 82 0.8328182585753107\n",
      "3 3 83 0.7489519880399803\n",
      "3 3 84 0.8498185336891854\n",
      "3 3 85 0.8397776633099457\n",
      "3 3 86 0.8473552034227787\n",
      "3 3 87 0.7804244491041123\n",
      "3 3 88 0.8276802076071151\n",
      "3 3 89 0.8568372244553663\n",
      "3 3 90 0.8875788743089653\n",
      "3 3 91 0.8339973035735422\n",
      "3 3 92 0.7722023757770029\n",
      "3 3 93 0.7648075019568277\n",
      "3 3 94 0.7819200670770459\n",
      "3 3 95 0.7368684906205849\n",
      "3 3 96 0.8018390763243507\n",
      "3 3 97 0.8770189376812176\n",
      "3 3 98 0.8480425747252934\n",
      "3 3 99 0.7782645718124782\n"
     ]
    }
   ],
   "source": [
    "n=1000\n",
    "for model1 in range(1,4):\n",
    "    for model2 in range(1,4):\n",
    "        #if not os.path.exists(\"./results-BENN/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n)): \n",
    "            #os.makedirs(\"./results-BENN/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n)) \n",
    "        dcor_list=[]\n",
    "        for t in range(1,101):\n",
    "            x_train=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/x_train_\" + str(t) + \".csv\")\n",
    "            x_train=x_train.drop('Unnamed: 0', axis=1)\n",
    "            y_train=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_train_\" + str(t) + \".csv\")\n",
    "            y_train=y_train.drop('Unnamed: 0', axis=1)\n",
    "            x_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/x_test_\" + str(t) + \".csv\")\n",
    "            x_test=x_test.drop('Unnamed: 0', axis=1)\n",
    "            y_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_test_\" + str(t) + \".csv\")\n",
    "            y_test=y_test.drop('Unnamed: 0', axis=1)\n",
    "            z_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/z_test_\" + str(t) + \".csv\")\n",
    "            z_test=z_test.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "\n",
    "            n=x_train.shape[0]\n",
    "            p=x_train.shape[1]\n",
    "            res_d=1\n",
    "            m=2\n",
    "            x_train = torch.tensor(x_train.values).to(torch.float)\n",
    "            y_train = torch.tensor(y_train.values).to(torch.float)\n",
    "            x_test = torch.tensor(x_test.values).to(torch.float)\n",
    "            y_test = torch.tensor(y_test.values).to(torch.float)\n",
    "            #print(x_train[:5],y_trans[:5])\n",
    "            mse_loss = nn.MSELoss()\n",
    "            # Build model\n",
    "            class nn_dr_reg_model(nn.Module):\n",
    "                def __init__(self, input_features, output_features, dim_red_features, hidden_units_d, hidden_units_e, dim_red_layers, ens_reg_layers):\n",
    "                    super().__init__()\n",
    "                    model_dim_red=[]\n",
    "                    model_dim_red.append(nn.Linear(in_features=input_features, \n",
    "                                                out_features=hidden_units_d))\n",
    "                    model_dim_red.append(nn.ReLU())\n",
    "                    for i in range(1,dim_red_layers):\n",
    "                        model_dim_red.append(nn.Linear(in_features=hidden_units_d, \n",
    "                                                    out_features=hidden_units_d))\n",
    "                        model_dim_red.append(nn.ReLU())\n",
    "                    model_dim_red.append(nn.Linear(in_features=hidden_units_d, \n",
    "                                                out_features=dim_red_features))\n",
    "                    self.dim_red_layer_stack = nn.Sequential(*model_dim_red)\n",
    "\n",
    "                    model_ens_reg=[]\n",
    "                    model_ens_reg.append(nn.Linear(in_features=dim_red_features, out_features=hidden_units_e))\n",
    "                    model_ens_reg.append(nn.ReLU())\n",
    "                    for i in range(1,ens_reg_layers):\n",
    "                        model_ens_reg.append(nn.Linear(in_features=hidden_units_e, out_features=hidden_units_e))\n",
    "                        model_ens_reg.append(nn.ReLU())\n",
    "                    model_ens_reg.append(nn.Linear(in_features=hidden_units_e, out_features=output_features))\n",
    "                    self.ens_reg_layer_stack = nn.Sequential(*model_ens_reg)\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    suff_predictor = self.dim_red_layer_stack(x)\n",
    "                    ens_output = self.ens_reg_layer_stack(suff_predictor)\n",
    "                    return ens_output, suff_predictor\n",
    "                    \n",
    "\n",
    "            # Create an instance of BlobModel and send it to the target device\n",
    "            model_nn = nn_dr_reg_model(input_features=p, \n",
    "                                    output_features=1, \n",
    "                                    dim_red_features=res_d, \n",
    "                                    hidden_units_d=200,\n",
    "                                    hidden_units_e=200,\n",
    "                                    dim_red_layers=4, \n",
    "                                    ens_reg_layers=4\n",
    "                                    ).to(device)\n",
    "            model_nn\n",
    "            optimizer = torch.optim.Adam(model_nn.parameters(), \n",
    "                                        lr=0.001)\n",
    "            epochs = 200\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            for epoch in range(epochs):\n",
    "                ### Training\n",
    "                model_nn.train()\n",
    "\n",
    "                # 1. Forward pass\n",
    "                y_pred_train, y_suff_train = model_nn(x_train) \n",
    "                \n",
    "                # 2. Calculate loss and accuracy\n",
    "                loss = mse_loss(y_pred_train, y_train) \n",
    "                \n",
    "                # 3. Optimizer zero grad\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 4. Loss backwards\n",
    "                loss.backward()\n",
    "\n",
    "                # 5. Optimizer step\n",
    "                optimizer.step()\n",
    "\n",
    "                ### Testing\n",
    "                model_nn.eval()\n",
    "\n",
    "                y_pred_test, y_suff_test = model_nn(x_test)\n",
    "                loss_test = mse_loss(y_pred_test, y_test) \n",
    "                #dcor_test = dcor.distance_correlation(y_suff_test.detach().numpy(),z_test)\n",
    "\n",
    "                if epoch % 25 == 0:\n",
    "                    #print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Test Loss: {loss_test:.5f} | Dcor: {dcor_test:.5f}\")\n",
    "                    now = datetime.now()\n",
    "                    current_time = now.strftime(\"%H:%M:%S\")\n",
    "                    #print(\"Current Time =\", current_time)\n",
    "            model_nn.eval()\n",
    "            with torch.inference_mode():\n",
    "                y_pred_test, y_suff_test = model_nn(x_test)\n",
    "            y_suff_test=y_suff_test.numpy()\n",
    "            dcor_current=dcor.distance_correlation(np.float64(y_suff_test),np.float64(z_test.to_numpy()))\n",
    "            dcor_list.append(dcor_current)\n",
    "            print(model1, model2, t, dcor_current)\n",
    "        dcor_list_df=pd.DataFrame(dcor_list)\n",
    "        dcor_list_df.to_csv(\"./results-BENN/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \".csv\")\n",
    "        print(model1, model2, np.mean(dcor_list), np.std(dcor_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db139c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ddabb-5fa4-44ca-9454-c7642ee55c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(y_suff_test,z_test,\"o\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd5fea4-2807-4390-85b4-71cf51773ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_suff_test_df=pd.DataFrame(y_suff_test)\n",
    "#y_pred_test_df=pd.DataFrame(y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c269471-400e-4f86-bc04-43d4321a7cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dcor.distance_correlation(y_suff_test,z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e797824b-bcda-4b59-bd56-d6036223d08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dcor.distance_correlation(np.float64(y_suff_test),np.float64(z_test.to_numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548afea9-f5c5-4fa5-a476-57affbae7fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bde6300-5115-407e-882f-9d834941dd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_conda",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
