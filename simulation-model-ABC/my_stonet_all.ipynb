{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de454e8-5aa6-49bc-bc91-04b6e707efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import errno\n",
    "import torch.utils.data\n",
    "from model import Net, Resize\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn import svm\n",
    "#from sklearn import metrics\n",
    "#from process_data import preprocess_data\n",
    "from train_stonet import train_stonet\n",
    "from model import DNN\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "from tools import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import dcor\n",
    "\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42, workers=True)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea1a0f42-5258-4733-87a9-3b0c22fee3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15, 1, 1]\n",
      "1 1 1000\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Running StoNet\")\n",
    "parser.add_argument('--seed', default=1, type = int, help = 'set seed')\n",
    "parser.add_argument('--data_path', default=\"./data/\", type=str, help='folder name for loading data')\n",
    "parser.add_argument('--base_path', default='./result/', type = str, help = 'base path for saving result')\n",
    "parser.add_argument('--model_path', default='./', type=str, help='folder name for saving model')\n",
    "parser.add_argument('--data_index', default=0, type=int)\n",
    "parser.add_argument('--data_name', default='waveform', type=str, help='the name of the dataset')\n",
    "parser.add_argument('--device', default='cpu', type=str, help='the device used to run the experiment')\n",
    "parser.add_argument('--model1', default=1, type = int, help = 'model1')\n",
    "parser.add_argument('--model2', default=1, type = int, help = 'model2')\n",
    "parser.add_argument('--n', default=1000, type = int, help = 'n')\n",
    "parser.add_argument('--r', default=10, type = int, help = 'r')\n",
    "\n",
    "\n",
    "# Training Setting\n",
    "parser.add_argument('--num_epochs', default=60, type=int, help='total number of training epochs')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum in SGD')\n",
    "parser.add_argument('--weight_decay', default=0, type=float, help='weight decay in SGD')\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='batch size for training')\n",
    "parser.add_argument('--MH_step', default=25, type=int, help='SGLD step for imputation')\n",
    "parser.add_argument('--sigma_list', default=[1e-3, 1e-6], nargs='+', type=float, help='list of sigma for stonet')\n",
    "parser.add_argument('--alpha', default=0.1, type=float, help='alpha for HMC')\n",
    "parser.add_argument('--proposal_lr', default=[7e-4, 7e-4], nargs='+', type=float, help='learning rate for training stonet')\n",
    "parser.add_argument('--step_size', default=0.01, type=float, help='learning rate for the optimizer')\n",
    "parser.add_argument('--net_architecture', default=[10, 1, 1], nargs='+', type=int, help='the architecture of the global stonet')\n",
    "parser.add_argument('--cross_validate_index', default=1, type=int, help='specify which fold of 5 fold cross validation')\n",
    "parser.add_argument('--regression_flag', default=1, type=int, help='1 for regression and 0 for classification')\n",
    "parser.add_argument('--confidence_interval_flag', default=False, type=int, help='whether to store result to compute confidence interval')\n",
    "parser.add_argument('--model_type', default='Logistic', type=str, help=\"the model to fit on the dimension reduced data including: 'Logistic', 'SVM', 'DNN'\")\n",
    "parser.add_argument('--activation', default='Tanh', type=str, help=\"the type of the activation function used in StoNet including: 'ReLU', 'Tanh'\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = args.device\n",
    "seed = args.seed\n",
    "data_name = args.data_name\n",
    "base_path = args.base_path\n",
    "model_path = args.model_path\n",
    "cross_validate_index = args.cross_validate_index\n",
    "regression_flag = args.regression_flag\n",
    "num_epochs = args.num_epochs\n",
    "model_path = args.model_path\n",
    "proposal_lr = args.proposal_lr\n",
    "step_size = args.step_size\n",
    "sigma_list = args.sigma_list\n",
    "alpha = args.alpha\n",
    "MH_step = args.MH_step\n",
    "momentum = args.momentum\n",
    "weight_decay = args.weight_decay\n",
    "batch_size = args.batch_size\n",
    "net_architecture = [args.r, 1, 1]\n",
    "num_hidden = len(net_architecture) - 1\n",
    "model_type = args.model_type\n",
    "activation = args.activation\n",
    "print(net_architecture)\n",
    "\n",
    "model1 = args.model1\n",
    "model2 = args.model2\n",
    "n = args.n\n",
    "print(model1, model2, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f11cf265-a3fd-4e4e-a791-678392dcd33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "train loss:  tensor(1.2993) ; train corr:  0.7404373376486417\n",
      "test loss:  tensor(1.4588) ; test corr:  0.7157816399383585\n",
      "epoch:  1\n",
      "train loss:  tensor(0.8368) ; train corr:  0.8475896735113944\n",
      "test loss:  tensor(0.9537) ; test corr:  0.8300211397006326\n",
      "epoch:  2\n",
      "train loss:  tensor(0.7160) ; train corr:  0.8763126140477729\n",
      "test loss:  tensor(0.8345) ; test corr:  0.8581450829398178\n",
      "epoch:  3\n",
      "train loss:  tensor(0.7641) ; train corr:  0.8651531798502743\n",
      "test loss:  tensor(0.8902) ; test corr:  0.8479610530923202\n",
      "epoch:  4\n",
      "train loss:  tensor(0.8078) ; train corr:  0.8475085575098323\n",
      "test loss:  tensor(0.9408) ; test corr:  0.827312977652388\n",
      "epoch:  5\n",
      "train loss:  tensor(0.6651) ; train corr:  0.8800373567982193\n",
      "test loss:  tensor(0.8237) ; test corr:  0.85277856464526\n",
      "epoch:  6\n",
      "train loss:  tensor(0.6844) ; train corr:  0.8731262695647604\n",
      "test loss:  tensor(0.8440) ; test corr:  0.8460043389357624\n",
      "epoch:  7\n",
      "train loss:  tensor(0.7705) ; train corr:  0.8691607634945088\n",
      "test loss:  tensor(0.9098) ; test corr:  0.8470409844022331\n",
      "epoch:  8\n",
      "train loss:  tensor(0.7630) ; train corr:  0.8550492138711137\n",
      "test loss:  tensor(0.9252) ; test corr:  0.8286624011602337\n",
      "epoch:  9\n",
      "train loss:  tensor(0.6310) ; train corr:  0.8871636421619502\n",
      "test loss:  tensor(0.7978) ; test corr:  0.8583995831354001\n",
      "epoch:  10\n",
      "train loss:  tensor(0.7367) ; train corr:  0.8645771903535397\n",
      "test loss:  tensor(0.9041) ; test corr:  0.8386607143930729\n",
      "epoch:  11\n",
      "train loss:  tensor(0.6440) ; train corr:  0.8831496041204044\n",
      "test loss:  tensor(0.8227) ; test corr:  0.8516594272782493\n",
      "epoch:  12\n",
      "train loss:  tensor(0.6575) ; train corr:  0.8831661341112326\n",
      "test loss:  tensor(0.8358) ; test corr:  0.8526750072325634\n",
      "epoch:  13\n",
      "train loss:  tensor(0.7739) ; train corr:  0.8654197515152537\n",
      "test loss:  tensor(0.9539) ; test corr:  0.8373502729840573\n",
      "epoch:  14\n",
      "train loss:  tensor(0.6923) ; train corr:  0.8699008929499397\n",
      "test loss:  tensor(0.8678) ; test corr:  0.8402827233422875\n",
      "epoch:  15\n",
      "train loss:  tensor(0.6686) ; train corr:  0.876424137505925\n",
      "test loss:  tensor(0.8487) ; test corr:  0.8458696013403759\n",
      "epoch:  16\n",
      "train loss:  tensor(0.7125) ; train corr:  0.8707445333644614\n",
      "test loss:  tensor(0.9040) ; test corr:  0.8375630387909565\n",
      "epoch:  17\n",
      "train loss:  tensor(0.7195) ; train corr:  0.8748025892131758\n",
      "test loss:  tensor(0.8801) ; test corr:  0.8485920045177393\n",
      "epoch:  18\n",
      "train loss:  tensor(0.8649) ; train corr:  0.8490547237591217\n",
      "test loss:  tensor(1.0468) ; test corr:  0.8206506918002804\n",
      "epoch:  19\n",
      "train loss:  tensor(0.7328) ; train corr:  0.8718162522197596\n",
      "test loss:  tensor(0.9435) ; test corr:  0.8333790718081697\n",
      "epoch:  20\n",
      "train loss:  tensor(0.7271) ; train corr:  0.8820261286782872\n",
      "test loss:  tensor(0.9100) ; test corr:  0.848873578609817\n",
      "epoch:  21\n",
      "train loss:  tensor(0.6981) ; train corr:  0.8709499378902533\n",
      "test loss:  tensor(0.9122) ; test corr:  0.8334894416436311\n",
      "epoch:  22\n",
      "train loss:  tensor(0.7329) ; train corr:  0.8659117737885171\n",
      "test loss:  tensor(0.9435) ; test corr:  0.8302732710761503\n",
      "epoch:  23\n",
      "train loss:  tensor(0.6601) ; train corr:  0.879556004531202\n",
      "test loss:  tensor(0.8619) ; test corr:  0.8430812732652682\n",
      "epoch:  24\n",
      "train loss:  tensor(0.6587) ; train corr:  0.8772327301241337\n",
      "test loss:  tensor(0.8600) ; test corr:  0.8418417037408599\n",
      "epoch:  25\n",
      "train loss:  tensor(0.7837) ; train corr:  0.8644386189299448\n",
      "test loss:  tensor(0.9477) ; test corr:  0.8380471662131225\n",
      "epoch:  26\n",
      "train loss:  tensor(0.7617) ; train corr:  0.8563551926809565\n",
      "test loss:  tensor(0.9493) ; test corr:  0.8254209833913786\n",
      "epoch:  27\n",
      "train loss:  tensor(0.7103) ; train corr:  0.8703676735635004\n",
      "test loss:  tensor(0.9139) ; test corr:  0.8359606179224458\n",
      "epoch:  28\n",
      "train loss:  tensor(0.6702) ; train corr:  0.8752161978394174\n",
      "test loss:  tensor(0.8537) ; test corr:  0.843422559352662\n",
      "epoch:  29\n",
      "train loss:  tensor(0.8037) ; train corr:  0.8579759158693459\n",
      "test loss:  tensor(0.9935) ; test corr:  0.8258048368026175\n",
      "epoch:  30\n",
      "train loss:  tensor(0.7685) ; train corr:  0.85453898890979\n",
      "test loss:  tensor(0.9627) ; test corr:  0.8213630355855713\n",
      "epoch:  31\n",
      "train loss:  tensor(0.6498) ; train corr:  0.880053275000964\n",
      "test loss:  tensor(0.8517) ; test corr:  0.8442867110471547\n",
      "epoch:  32\n",
      "train loss:  tensor(0.6955) ; train corr:  0.8725565010396188\n",
      "test loss:  tensor(0.8956) ; test corr:  0.8377095211420467\n",
      "epoch:  33\n",
      "train loss:  tensor(0.8085) ; train corr:  0.8642133421485553\n",
      "test loss:  tensor(1.0203) ; test corr:  0.8286800290668636\n",
      "epoch:  34\n",
      "train loss:  tensor(0.7009) ; train corr:  0.8684273646802501\n",
      "test loss:  tensor(0.9156) ; test corr:  0.8304567145824047\n",
      "epoch:  35\n",
      "train loss:  tensor(0.7403) ; train corr:  0.8677593226148781\n",
      "test loss:  tensor(0.9582) ; test corr:  0.8286392252186803\n",
      "epoch:  36\n",
      "train loss:  tensor(0.7213) ; train corr:  0.8636461875538411\n",
      "test loss:  tensor(0.9590) ; test corr:  0.8211161872676572\n",
      "epoch:  37\n",
      "train loss:  tensor(0.6742) ; train corr:  0.8746412085789886\n",
      "test loss:  tensor(0.8784) ; test corr:  0.8387438233005745\n",
      "epoch:  38\n",
      "train loss:  tensor(0.8510) ; train corr:  0.8650543035842669\n",
      "test loss:  tensor(1.0730) ; test corr:  0.8258075541986949\n",
      "epoch:  39\n",
      "train loss:  tensor(0.7071) ; train corr:  0.866824104350253\n",
      "test loss:  tensor(0.9368) ; test corr:  0.8257339750383539\n",
      "epoch:  40\n",
      "train loss:  tensor(0.7288) ; train corr:  0.8767562693524029\n",
      "test loss:  tensor(0.9477) ; test corr:  0.8357567727984906\n",
      "epoch:  41\n",
      "train loss:  tensor(0.7111) ; train corr:  0.865876596800746\n",
      "test loss:  tensor(0.9446) ; test corr:  0.8244602702329085\n",
      "epoch:  42\n",
      "train loss:  tensor(0.6750) ; train corr:  0.8758325512202453\n",
      "test loss:  tensor(0.8995) ; test corr:  0.8345931163866542\n",
      "epoch:  43\n",
      "train loss:  tensor(0.7144) ; train corr:  0.8706747811695278\n",
      "test loss:  tensor(0.9300) ; test corr:  0.832565671625464\n",
      "epoch:  44\n",
      "train loss:  tensor(0.6450) ; train corr:  0.8826505780213948\n",
      "test loss:  tensor(0.8713) ; test corr:  0.8422315774986607\n",
      "epoch:  45\n",
      "train loss:  tensor(0.7852) ; train corr:  0.870322058976687\n",
      "test loss:  tensor(0.9842) ; test corr:  0.8347964069142805\n",
      "epoch:  46\n",
      "train loss:  tensor(0.7763) ; train corr:  0.8550649601436855\n",
      "test loss:  tensor(0.9930) ; test corr:  0.816379177453645\n",
      "epoch:  47\n",
      "train loss:  tensor(0.7045) ; train corr:  0.8685499632302794\n",
      "test loss:  tensor(0.9536) ; test corr:  0.823528730313415\n",
      "epoch:  48\n",
      "train loss:  tensor(0.6777) ; train corr:  0.8753000884990615\n",
      "test loss:  tensor(0.8763) ; test corr:  0.8404117052507609\n",
      "epoch:  49\n",
      "train loss:  tensor(0.7348) ; train corr:  0.8650187891623565\n",
      "test loss:  tensor(0.9669) ; test corr:  0.8235821666245459\n",
      "epoch:  50\n",
      "train loss:  tensor(0.6542) ; train corr:  0.8797729284666522\n",
      "test loss:  tensor(0.9107) ; test corr:  0.831786240025333\n",
      "epoch:  51\n",
      "train loss:  tensor(0.6998) ; train corr:  0.8794550704389037\n",
      "test loss:  tensor(0.9215) ; test corr:  0.839232894285205\n",
      "epoch:  52\n",
      "train loss:  tensor(0.6777) ; train corr:  0.8738564658248266\n",
      "test loss:  tensor(0.8885) ; test corr:  0.8363084872212205\n",
      "epoch:  53\n",
      "train loss:  tensor(0.6844) ; train corr:  0.8814978664906129\n",
      "test loss:  tensor(0.8945) ; test corr:  0.845001047728245\n",
      "epoch:  54\n",
      "train loss:  tensor(0.6857) ; train corr:  0.874176686124628\n",
      "test loss:  tensor(0.8972) ; test corr:  0.8370608295786883\n",
      "epoch:  55\n",
      "train loss:  tensor(0.6136) ; train corr:  0.8884556413954126\n",
      "test loss:  tensor(0.8420) ; test corr:  0.8467219334063933\n",
      "epoch:  56\n",
      "train loss:  tensor(0.6599) ; train corr:  0.8856092164314939\n",
      "test loss:  tensor(0.8775) ; test corr:  0.8467655080088075\n",
      "epoch:  57\n",
      "train loss:  tensor(0.6938) ; train corr:  0.8746499908846194\n",
      "test loss:  tensor(0.9192) ; test corr:  0.8341821379834585\n",
      "epoch:  58\n",
      "train loss:  tensor(0.6483) ; train corr:  0.8816428230431443\n",
      "test loss:  tensor(0.8842) ; test corr:  0.8388569760743595\n",
      "epoch:  59\n",
      "train loss:  tensor(0.7607) ; train corr:  0.8833017587884621\n",
      "test loss:  tensor(0.9941) ; test corr:  0.8413184088241059\n",
      "apply activation: Tanh\n",
      "1 1 1 0.9103993066503131\n",
      "epoch:  0\n",
      "train loss:  tensor(1.1256) ; train corr:  0.7492762387924615\n",
      "test loss:  tensor(1.3550) ; test corr:  0.7099274920931482\n",
      "epoch:  1\n",
      "train loss:  tensor(0.7865) ; train corr:  0.834085169771638\n",
      "test loss:  tensor(0.9657) ; test corr:  0.8084964879788223\n",
      "epoch:  2\n",
      "train loss:  tensor(0.6673) ; train corr:  0.8581347999489733\n",
      "test loss:  tensor(0.8705) ; test corr:  0.8262692006382168\n",
      "epoch:  3\n",
      "train loss:  tensor(0.6611) ; train corr:  0.8580517196488805\n",
      "test loss:  tensor(0.9030) ; test corr:  0.817329329805136\n",
      "epoch:  4\n",
      "train loss:  tensor(0.6180) ; train corr:  0.8724243045046457\n",
      "test loss:  tensor(0.8543) ; test corr:  0.8303314011395728\n",
      "epoch:  5\n",
      "train loss:  tensor(0.6500) ; train corr:  0.860445130709566\n",
      "test loss:  tensor(0.8862) ; test corr:  0.8203165821549381\n",
      "epoch:  6\n",
      "train loss:  tensor(0.6224) ; train corr:  0.8676092142587649\n",
      "test loss:  tensor(0.8574) ; test corr:  0.8264826171667576\n",
      "epoch:  7\n",
      "train loss:  tensor(0.6275) ; train corr:  0.8663402301874709\n",
      "test loss:  tensor(0.8577) ; test corr:  0.8261719136445969\n",
      "epoch:  8\n",
      "train loss:  tensor(0.5835) ; train corr:  0.875950405929461\n",
      "test loss:  tensor(0.8374) ; test corr:  0.8302385721435018\n",
      "epoch:  9\n",
      "train loss:  tensor(0.5794) ; train corr:  0.8772113696595256\n",
      "test loss:  tensor(0.8417) ; test corr:  0.8293779112483131\n",
      "epoch:  10\n",
      "train loss:  tensor(0.6009) ; train corr:  0.8738286777266325\n",
      "test loss:  tensor(0.8929) ; test corr:  0.8222631380678758\n",
      "epoch:  11\n",
      "train loss:  tensor(0.5953) ; train corr:  0.8790827767558625\n",
      "test loss:  tensor(0.8409) ; test corr:  0.8339371860951975\n",
      "epoch:  12\n",
      "train loss:  tensor(0.5934) ; train corr:  0.8736926241587141\n",
      "test loss:  tensor(0.8699) ; test corr:  0.8239249870862383\n",
      "epoch:  13\n",
      "train loss:  tensor(0.5559) ; train corr:  0.8857886155886204\n",
      "test loss:  tensor(0.8496) ; test corr:  0.8279906750068244\n",
      "epoch:  14\n",
      "train loss:  tensor(0.5861) ; train corr:  0.8765336505399639\n",
      "test loss:  tensor(0.8898) ; test corr:  0.8217398594197037\n",
      "epoch:  15\n",
      "train loss:  tensor(0.5625) ; train corr:  0.8835306492770342\n",
      "test loss:  tensor(0.8510) ; test corr:  0.8285238480484856\n",
      "epoch:  16\n",
      "train loss:  tensor(0.5453) ; train corr:  0.8855264760672404\n",
      "test loss:  tensor(0.8660) ; test corr:  0.8239940404021551\n",
      "epoch:  17\n",
      "train loss:  tensor(0.5588) ; train corr:  0.8819341554491635\n",
      "test loss:  tensor(0.8897) ; test corr:  0.8191689501449932\n",
      "epoch:  18\n",
      "train loss:  tensor(0.5651) ; train corr:  0.8796222104420184\n",
      "test loss:  tensor(0.9068) ; test corr:  0.8171256105427016\n",
      "epoch:  19\n",
      "train loss:  tensor(0.5757) ; train corr:  0.8770727698604864\n",
      "test loss:  tensor(0.9145) ; test corr:  0.8145320070687134\n",
      "epoch:  20\n",
      "train loss:  tensor(0.6000) ; train corr:  0.8715959539801871\n",
      "test loss:  tensor(0.9256) ; test corr:  0.8113464240894583\n",
      "epoch:  21\n",
      "train loss:  tensor(0.6282) ; train corr:  0.870678404954364\n",
      "test loss:  tensor(0.9307) ; test corr:  0.8139636825602597\n",
      "epoch:  22\n",
      "train loss:  tensor(0.5771) ; train corr:  0.8770611064750579\n",
      "test loss:  tensor(0.9301) ; test corr:  0.8118633408207421\n",
      "epoch:  23\n",
      "train loss:  tensor(0.5961) ; train corr:  0.8766111653895258\n",
      "test loss:  tensor(0.8950) ; test corr:  0.8199584404841802\n",
      "epoch:  24\n",
      "train loss:  tensor(0.5924) ; train corr:  0.8731972579594034\n",
      "test loss:  tensor(0.9286) ; test corr:  0.812181954883635\n",
      "epoch:  25\n",
      "train loss:  tensor(0.5800) ; train corr:  0.8776455657171405\n",
      "test loss:  tensor(0.9021) ; test corr:  0.816930518301913\n",
      "epoch:  26\n",
      "train loss:  tensor(0.6064) ; train corr:  0.8715031014132607\n",
      "test loss:  tensor(0.9376) ; test corr:  0.8114035467836579\n",
      "epoch:  27\n",
      "train loss:  tensor(0.5890) ; train corr:  0.8741843677223828\n",
      "test loss:  tensor(0.9286) ; test corr:  0.8122186923747825\n",
      "epoch:  28\n",
      "train loss:  tensor(0.5980) ; train corr:  0.8744047285475273\n",
      "test loss:  tensor(0.9158) ; test corr:  0.8144473027013203\n",
      "epoch:  29\n",
      "train loss:  tensor(0.5956) ; train corr:  0.8724844941907796\n",
      "test loss:  tensor(0.9587) ; test corr:  0.8064557968094272\n",
      "epoch:  30\n",
      "train loss:  tensor(0.6074) ; train corr:  0.8737065816379747\n",
      "test loss:  tensor(0.9181) ; test corr:  0.8148256481086252\n",
      "epoch:  31\n",
      "train loss:  tensor(0.6057) ; train corr:  0.8710789821240027\n",
      "test loss:  tensor(0.9517) ; test corr:  0.8080839242816903\n",
      "epoch:  32\n",
      "train loss:  tensor(0.5817) ; train corr:  0.8759852263266003\n",
      "test loss:  tensor(0.9499) ; test corr:  0.8068960311689808\n",
      "epoch:  33\n",
      "train loss:  tensor(0.6716) ; train corr:  0.8585466745028603\n",
      "test loss:  tensor(1.0725) ; test corr:  0.7888050316308846\n",
      "epoch:  34\n",
      "train loss:  tensor(0.7356) ; train corr:  0.8407308254341213\n",
      "test loss:  tensor(1.0591) ; test corr:  0.7838039123782136\n",
      "epoch:  35\n",
      "train loss:  tensor(0.6887) ; train corr:  0.8580306725677813\n",
      "test loss:  tensor(1.0133) ; test corr:  0.7991678915783292\n",
      "epoch:  36\n",
      "train loss:  tensor(0.6720) ; train corr:  0.8571153245147738\n",
      "test loss:  tensor(1.0074) ; test corr:  0.797241726342597\n",
      "epoch:  37\n",
      "train loss:  tensor(0.6362) ; train corr:  0.8633134322619023\n",
      "test loss:  tensor(0.9833) ; test corr:  0.7998494244704395\n",
      "epoch:  38\n",
      "train loss:  tensor(0.6435) ; train corr:  0.8620273282665514\n",
      "test loss:  tensor(1.0139) ; test corr:  0.7941893049440178\n",
      "epoch:  39\n",
      "train loss:  tensor(0.6278) ; train corr:  0.8668228821536886\n",
      "test loss:  tensor(0.9721) ; test corr:  0.8023307007224275\n",
      "epoch:  40\n",
      "train loss:  tensor(0.6186) ; train corr:  0.8699058160305629\n",
      "test loss:  tensor(0.9671) ; test corr:  0.8048833660651208\n",
      "epoch:  41\n",
      "train loss:  tensor(0.6354) ; train corr:  0.8652133766524259\n",
      "test loss:  tensor(0.9704) ; test corr:  0.8022608451241063\n",
      "epoch:  42\n",
      "train loss:  tensor(0.6321) ; train corr:  0.8643509940809364\n",
      "test loss:  tensor(1.0009) ; test corr:  0.7977345676202394\n",
      "epoch:  43\n",
      "train loss:  tensor(0.6718) ; train corr:  0.856062161812777\n",
      "test loss:  tensor(1.0419) ; test corr:  0.790125749104165\n",
      "epoch:  44\n",
      "train loss:  tensor(0.6399) ; train corr:  0.8682948498220219\n",
      "test loss:  tensor(0.9798) ; test corr:  0.8020238293776619\n",
      "epoch:  45\n",
      "train loss:  tensor(0.6406) ; train corr:  0.8627910369705691\n",
      "test loss:  tensor(1.0073) ; test corr:  0.7967916780502713\n",
      "epoch:  46\n",
      "train loss:  tensor(0.6735) ; train corr:  0.8585289450256922\n",
      "test loss:  tensor(1.0219) ; test corr:  0.7921799959613148\n",
      "epoch:  47\n",
      "train loss:  tensor(0.6434) ; train corr:  0.8629692757529839\n",
      "test loss:  tensor(0.9835) ; test corr:  0.7989068241994073\n",
      "epoch:  48\n",
      "train loss:  tensor(0.6576) ; train corr:  0.858352320267919\n",
      "test loss:  tensor(1.0330) ; test corr:  0.7895200117087579\n",
      "epoch:  49\n",
      "train loss:  tensor(0.6583) ; train corr:  0.8583785872139859\n",
      "test loss:  tensor(1.0841) ; test corr:  0.778301159697616\n",
      "epoch:  50\n",
      "train loss:  tensor(0.6814) ; train corr:  0.8616781181056248\n",
      "test loss:  tensor(1.0343) ; test corr:  0.7930511695599209\n",
      "epoch:  51\n",
      "train loss:  tensor(0.6632) ; train corr:  0.8584076830928338\n",
      "test loss:  tensor(1.1187) ; test corr:  0.7778101796198195\n",
      "epoch:  52\n",
      "train loss:  tensor(0.7103) ; train corr:  0.8598187131147095\n",
      "test loss:  tensor(1.0623) ; test corr:  0.7902320983876246\n",
      "epoch:  53\n",
      "train loss:  tensor(0.6515) ; train corr:  0.8601361321053055\n",
      "test loss:  tensor(1.0187) ; test corr:  0.790448203658379\n",
      "epoch:  54\n",
      "train loss:  tensor(0.6645) ; train corr:  0.8571531278807106\n",
      "test loss:  tensor(1.0158) ; test corr:  0.7924387067949485\n",
      "epoch:  55\n",
      "train loss:  tensor(0.6982) ; train corr:  0.8581215816102422\n",
      "test loss:  tensor(1.0073) ; test corr:  0.7986012527577991\n",
      "epoch:  56\n",
      "train loss:  tensor(0.6120) ; train corr:  0.8698218003001269\n",
      "test loss:  tensor(1.0151) ; test corr:  0.791589898423844\n",
      "epoch:  57\n",
      "train loss:  tensor(0.6504) ; train corr:  0.8638509781193381\n",
      "test loss:  tensor(0.9885) ; test corr:  0.7994127002900722\n",
      "epoch:  58\n",
      "train loss:  tensor(0.7022) ; train corr:  0.8532655843777137\n",
      "test loss:  tensor(1.0316) ; test corr:  0.790957385947728\n",
      "epoch:  59\n",
      "train loss:  tensor(0.7053) ; train corr:  0.851223769904231\n",
      "test loss:  tensor(1.1009) ; test corr:  0.7817820873315465\n",
      "apply activation: Tanh\n",
      "1 1 2 0.894138622213697\n",
      "1 1 0.902268964432005 0.008130342218308051\n"
     ]
    }
   ],
   "source": [
    "dcor_list=[]\n",
    "for t in range(1,101):\n",
    "    x_train=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/x_train_\" + str(t) + \".csv\")\n",
    "    x_train=x_train.drop('Unnamed: 0', axis=1)\n",
    "    y_train=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_train_\" + str(t) + \".csv\")\n",
    "    y_train=y_train.drop('Unnamed: 0', axis=1)\n",
    "    x_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/x_test_\" + str(t) + \".csv\")\n",
    "    x_test=x_test.drop('Unnamed: 0', axis=1)\n",
    "    y_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_test_\" + str(t) + \".csv\")\n",
    "    y_test=y_test.drop('Unnamed: 0', axis=1)\n",
    "    z_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/z_test_\" + str(t) + \".csv\")\n",
    "    z_test=z_test.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    x_train = torch.FloatTensor(x_train.to_numpy()).to(device)\n",
    "    y_train = torch.FloatTensor(y_train.to_numpy()).to(device)\n",
    "    x_test = torch.FloatTensor(x_test.to_numpy()).to(device)\n",
    "    y_test = torch.FloatTensor(y_test.to_numpy()).to(device)\n",
    "\n",
    "    mis_rec = []\n",
    "\n",
    "\n",
    "    step_size=0.01\n",
    "        #print(\"cross: \", cross_validate_index)\n",
    "        # set random seed\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "        # load data\n",
    "        #x_train, y_train, x_test, y_test = preprocess_data(data_name, cross_validate_index)\n",
    "\n",
    "\n",
    "        #if data_name == 'MNIST':\n",
    "        #    x_train, x_test = x_train.view(x_train.size(0), -1), x_test.view(x_test.size(0), -1)\n",
    "        #    x_train, x_test, y_train, y_test = x_train[:20000], x_test[:20000], y_train[:20000], y_test[:20000]\n",
    "\n",
    "\n",
    "        ###### pretraining\n",
    "    #start_time = time.clock()\n",
    "    net, last_hidden_layer_train, last_hidden_layer_test = train_stonet(x_train, x_test, y_train, y_test,\n",
    "                PATH=base_path + data_name+'/reduce_dim_' + str(net_architecture[-2]) + '/cross_'+str(cross_validate_index)+'/', net_index=\"stonet\", net_architecture=net_architecture, num_epochs=num_epochs, \n",
    "                subn=batch_size, sigma_list=sigma_list, alpha=alpha, MH_step=MH_step, \n",
    "                proposal_lr=proposal_lr, step_size=step_size, momentum=momentum, weight_decay=weight_decay, device=device, regression_flag=regression_flag)\n",
    "    #time_elapse = time.clock() - start_time\n",
    "\n",
    "\n",
    "        ###### train the stonet with a good start\n",
    "        # set loss function\n",
    "    sse = nn.MSELoss(reduction='sum')\n",
    "    if regression_flag:\n",
    "        loss_func = nn.MSELoss()\n",
    "        loss_func_sum = nn.MSELoss(reduction='sum')\n",
    "        train_loss_path = np.zeros(num_epochs)\n",
    "        test_loss_path = np.zeros(num_epochs)\n",
    "        train_accuracy_path = np.zeros(num_epochs)\n",
    "        test_accuracy_path = np.zeros(num_epochs)\n",
    "    else:\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss_func_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "        train_loss_path = np.zeros(num_epochs)\n",
    "        test_loss_path = np.zeros(num_epochs)\n",
    "        train_accuracy_path = np.zeros(num_epochs)\n",
    "        test_accuracy_path = np.zeros(num_epochs)\n",
    "\n",
    "    sigma_output = sigma_list[-1]\n",
    "    ntrain = x_train.shape[0]\n",
    "    ntest = x_test.shape[0]\n",
    "    num_block = len(net_architecture)\n",
    "    a = 0.6\n",
    "    step_size=0.001\n",
    "\n",
    "    for epoch in range(30):\n",
    "\n",
    "            # set optimizer\n",
    "        optimizer_list = []\n",
    "        for i in range(len(net_architecture)):\n",
    "            optimizer_list.append(torch.optim.SGD(net.block_list[i].parameters(), lr=step_size/(1+step_size*(epoch)**a), momentum=momentum, weight_decay=weight_decay))\n",
    "\n",
    "        hidden_list = []\n",
    "        momentum_list = []\n",
    "        with torch.no_grad():\n",
    "            hidden_list.append(net.block_list[0](x_train))\n",
    "            momentum_list.append(torch.zeros_like(hidden_list[-1]))\n",
    "            for i in range(1, num_hidden):\n",
    "                hidden_list.append(net.block_list[i](hidden_list[-1]))\n",
    "                momentum_list.append(torch.zeros_like(hidden_list[-1]))\n",
    "\n",
    "        foward_hidden = net.block_list[0](x_train).data\n",
    "\n",
    "        for i in range(hidden_list.__len__()):\n",
    "            hidden_list[i].requires_grad = True\n",
    "\n",
    "        for repeat in range(MH_step):\n",
    "            for layer_index in reversed(range(num_hidden)):\n",
    "                if hidden_list[layer_index].grad is not None:\n",
    "                    hidden_list[layer_index].grad.zero_()\n",
    "\n",
    "                if layer_index == num_hidden - 1:\n",
    "                    hidden_likelihood = -loss_func_sum(\n",
    "                        net.block_list[layer_index + 1](hidden_list[layer_index]),\n",
    "                        y_train) / sigma_output\n",
    "                else:\n",
    "                    hidden_likelihood = -sse(net.block_list[layer_index + 1](hidden_list[layer_index]),\n",
    "                                              hidden_list[layer_index + 1]) / sigma_list[layer_index + 1]\n",
    "                if layer_index == 0:\n",
    "                    hidden_likelihood = hidden_likelihood - sse(\n",
    "                        foward_hidden,\n",
    "                        hidden_list[layer_index]) / sigma_list[layer_index]\n",
    "                else:\n",
    "                    hidden_likelihood = hidden_likelihood - sse(\n",
    "                        net.block_list[layer_index](hidden_list[layer_index - 1]),\n",
    "                        hidden_list[layer_index]) / sigma_list[layer_index]\n",
    "\n",
    "                hidden_likelihood.backward()\n",
    "                step_proposal_lr = proposal_lr[layer_index]/(1+proposal_lr[layer_index]*(epoch)**a)\n",
    "                gamma = alpha / step_proposal_lr\n",
    "                with torch.no_grad():\n",
    "                    momentum_list[layer_index] = (1 - gamma * step_proposal_lr) * momentum_list[layer_index] + step_proposal_lr * hidden_list[\n",
    "                        layer_index].grad + torch.FloatTensor(\n",
    "                        hidden_list[layer_index].shape).to(device).normal_().mul(\n",
    "                        np.sqrt(2 * gamma * step_proposal_lr))\n",
    "                    hidden_list[layer_index].data += step_proposal_lr * momentum_list[layer_index]\n",
    "\n",
    "            num_optim_step = 1\n",
    "            for layer_index in range(num_block):\n",
    "                for step in range(num_optim_step):\n",
    "                    if layer_index == 0:\n",
    "                        loss = sse(net.block_list[layer_index](x_train),\n",
    "                                    hidden_list[layer_index]) / ntrain\n",
    "                    elif layer_index == num_block - 1:\n",
    "                        loss = loss_func_sum(net.block_list[layer_index](hidden_list[layer_index - 1]),\n",
    "                                                y_train) / ntrain\n",
    "                    else:\n",
    "                        loss = sse(net.block_list[layer_index](hidden_list[layer_index - 1]),\n",
    "                                    hidden_list[layer_index]) / ntrain\n",
    "                    optimizer_list[layer_index].zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer_list[layer_index].step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if regression_flag:\n",
    "                #print('epoch: ', epoch)\n",
    "\n",
    "                output, last_hidden_layer_train = net(x_train)\n",
    "                train_loss = loss_func(output, y_train)\n",
    "                train_loss_path[epoch] = train_loss\n",
    "                train_corr = np.corrcoef(output.squeeze(), y_train.squeeze())[0,1]\n",
    "                train_accuracy_path[epoch] = train_corr\n",
    "                #print(\"train loss: \", train_loss, \"; train corr: \", train_corr)\n",
    "\n",
    "                output, last_hidden_layer_test = net(x_test)\n",
    "                test_loss = loss_func(output, y_test)\n",
    "                test_loss_path[epoch] = test_loss\n",
    "                test_corr = np.corrcoef(output.squeeze(), y_test.squeeze())[0,1]\n",
    "                test_accuracy_path[epoch] = test_corr\n",
    "                #print(\"test loss: \", test_loss, \"; test corr: \", test_corr)\n",
    "\n",
    "            else:\n",
    "                print('epoch: ', epoch)\n",
    "                output, last_hidden_layer_train = net(x_train)\n",
    "                train_loss = loss_func(output, y_train)\n",
    "                prediction = output.data.max(1)[1]\n",
    "                train_accuracy = prediction.eq(y_train.data).sum().item() / ntrain\n",
    "\n",
    "                print(\"train loss: \", train_loss, 'train accuracy: ', train_accuracy)\n",
    "\n",
    "                output, last_hidden_layer_test = net(x_test)\n",
    "                test_loss = loss_func(output, y_test)\n",
    "                prediction = output.data.max(1)[1]\n",
    "                test_accuracy = prediction.eq(y_test.data).sum().item() / ntest\n",
    "\n",
    "                print(\"test loss: \", test_loss, 'test accuracy: ', test_accuracy)\n",
    "\n",
    "        # get the output of the last hidden layer\n",
    "    output, last_hidden_layer_train = net(x_train)\n",
    "    output, last_hidden_layer_test = net(x_test)\n",
    "\n",
    "\n",
    "\n",
    "    if activation == \"ReLU\":\n",
    "        print(\"apply activation: ReLU\")\n",
    "        m = nn.ReLU()\n",
    "    elif activation == \"Tanh\":\n",
    "        print(\"apply activation: Tanh\")\n",
    "        m = nn.Tanh()\n",
    "    last_hidden_layer_train = m(last_hidden_layer_train)\n",
    "    last_hidden_layer_test = m(last_hidden_layer_test)\n",
    "    last_hidden_layer_train = last_hidden_layer_train.detach().cpu().numpy()\n",
    "    last_hidden_layer_test = last_hidden_layer_test.detach().cpu().numpy()\n",
    "    dcor_current=dcor.distance_correlation(last_hidden_layer_test,z_test.to_numpy(),method=\"naive\")\n",
    "    dcor_list.append(dcor_current)\n",
    "    print(model1, model2, t, dcor_current)\n",
    "dcor_list_df=pd.DataFrame(dcor_list)\n",
    "dcor_list_df.to_csv(\"./results-StoNet-\" + str(net_architecture[0]) + \"/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \".csv\")\n",
    "print(model1, model2, np.mean(dcor_list), np.std(dcor_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdaccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d9a81-8337-4501-8757-c22acddbf72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01668c10-ac80-4cdb-bf86-85fe837ba226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_conda",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
