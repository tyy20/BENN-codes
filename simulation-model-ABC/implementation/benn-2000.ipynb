{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7484cc-6b31-4d27-b17b-b3a89569d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42, workers=True)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "\n",
    "import dcor\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "import os \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50359073-4520-42d2-bcc6-63ce3e3ca145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e0c665f-c218-4553-96f8-80fd29f1114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 0.9370502796106446\n",
      "1 1 2 0.9393286721792483\n",
      "1 1 3 0.9341674010060695\n",
      "1 1 4 0.9241937165845101\n",
      "1 1 5 0.9379997811378074\n",
      "1 1 6 0.9410960068007259\n",
      "1 1 7 0.932917382309664\n",
      "1 1 8 0.9298286487659617\n",
      "1 1 9 0.9409701329285639\n",
      "1 1 10 0.9449925544140583\n",
      "1 1 11 0.9438152436439751\n",
      "1 1 12 0.9371018414761393\n",
      "1 1 13 0.9408907248294786\n",
      "1 1 14 0.949552843173084\n",
      "1 1 15 0.9356455977358272\n",
      "1 1 16 0.9391286579792901\n",
      "1 1 17 0.933830011045357\n",
      "1 1 18 0.9333071686623762\n",
      "1 1 19 0.9359198241500771\n",
      "1 1 20 0.9347323434361753\n",
      "1 1 21 0.9465249398649703\n",
      "1 1 22 0.9442122778268046\n",
      "1 1 23 0.9354467427860673\n",
      "1 1 24 0.9344272641049706\n",
      "1 1 25 0.9382976962297943\n",
      "1 1 26 0.9216799230757045\n",
      "1 1 27 0.9315796697573493\n",
      "1 1 28 0.9354327493319733\n",
      "1 1 29 0.9326196322148275\n",
      "1 1 30 0.9540578681653719\n",
      "1 1 31 0.9331608812346783\n",
      "1 1 32 0.9446004829714543\n",
      "1 1 33 0.9424885785701645\n",
      "1 1 34 0.9332870447092849\n",
      "1 1 35 0.937452177009404\n",
      "1 1 36 0.9416315052949515\n",
      "1 1 37 0.9363239515092139\n",
      "1 1 38 0.9417920178229494\n",
      "1 1 39 0.9340305999990423\n",
      "1 1 40 0.9301127335666949\n",
      "1 1 41 0.9419933857767206\n",
      "1 1 42 0.9382904244985911\n",
      "1 1 43 0.9437556331713477\n",
      "1 1 44 0.9078203047584354\n",
      "1 1 45 0.9306183294363253\n",
      "1 1 46 0.9025652477926963\n",
      "1 1 47 0.9290256687928179\n",
      "1 1 48 0.9249277336306706\n",
      "1 1 49 0.9340790516209019\n",
      "1 1 50 0.9379046257085567\n",
      "1 1 51 0.9249061590583709\n",
      "1 1 52 0.9316026560126979\n",
      "1 1 53 0.9252526550956912\n",
      "1 1 54 0.936739286103097\n",
      "1 1 55 0.9254027470814706\n",
      "1 1 56 0.941607663148452\n",
      "1 1 57 0.9393640028997503\n",
      "1 1 58 0.9399565384269877\n",
      "1 1 59 0.9453536742066476\n",
      "1 1 60 0.9105715330284793\n",
      "1 1 61 0.9389258743712524\n",
      "1 1 62 0.9425906776199859\n",
      "1 1 63 0.9394647334431588\n",
      "1 1 64 0.9308725893279882\n",
      "1 1 65 0.946866293937176\n",
      "1 1 66 0.9361610889514125\n",
      "1 1 67 0.917331766005274\n",
      "1 1 68 0.942489752632353\n",
      "1 1 69 0.9442557843346022\n",
      "1 1 70 0.9498701431089936\n",
      "1 1 71 0.9425561623260882\n",
      "1 1 72 0.9096212588044751\n",
      "1 1 73 0.9325675527252812\n",
      "1 1 74 0.9443511051251512\n",
      "1 1 75 0.9470297633613929\n",
      "1 1 76 0.935647598245972\n",
      "1 1 77 0.9317914686277043\n",
      "1 1 78 0.919102625282934\n",
      "1 1 79 0.9333851874878374\n",
      "1 1 80 0.9197578438836911\n",
      "1 1 81 0.9457795669469762\n",
      "1 1 82 0.9241174026645547\n",
      "1 1 83 0.9248974503428387\n",
      "1 1 84 0.9318738258303773\n",
      "1 1 85 0.9320558701505289\n",
      "1 1 86 0.9427073096447355\n",
      "1 1 87 0.9454380110494491\n",
      "1 1 88 0.9352237358106102\n",
      "1 1 89 0.937867053816052\n",
      "1 1 90 0.9389751648448303\n",
      "1 1 91 0.9420861022259263\n",
      "1 1 92 0.9282285939322508\n",
      "1 1 93 0.9370218716360873\n",
      "1 1 94 0.940106461995195\n",
      "1 1 95 0.9472259543132799\n",
      "1 1 96 0.9286725478014646\n",
      "1 1 97 0.9463881656662738\n",
      "1 1 98 0.929227127386248\n",
      "1 1 99 0.944961971459231\n",
      "1 1 100 0.9356539118766205\n",
      "1 1 0.9355051225715966 0.009177021572000456\n",
      "1 2 1 0.9697753533016029\n",
      "1 2 2 0.9526639165473882\n",
      "1 2 3 0.9694180014165572\n",
      "1 2 4 0.9678418328392263\n",
      "1 2 5 0.9677518064909254\n",
      "1 2 6 0.9688244315757647\n",
      "1 2 7 0.9668690929433356\n",
      "1 2 8 0.967455938856675\n",
      "1 2 9 0.966229669344894\n",
      "1 2 10 0.9081167353038126\n",
      "1 2 11 0.9722224147312722\n",
      "1 2 12 0.9720865755138018\n",
      "1 2 13 0.9679346419861157\n",
      "1 2 14 0.9652390651348415\n",
      "1 2 15 0.9653085889439924\n",
      "1 2 16 0.9619594459656857\n",
      "1 2 17 0.9610482212261132\n",
      "1 2 18 0.9678345790123255\n",
      "1 2 19 0.9708343022462058\n",
      "1 2 20 0.9601132436665811\n",
      "1 2 21 0.9481958415381314\n",
      "1 2 22 0.9659354064000645\n",
      "1 2 23 0.9683722718118\n",
      "1 2 24 0.9633177969801129\n",
      "1 2 25 0.9701792291224454\n",
      "1 2 26 0.9659617208391577\n",
      "1 2 27 0.9727879207978081\n",
      "1 2 28 0.9625672003055948\n",
      "1 2 29 0.9717132997240345\n",
      "1 2 30 0.9635067397375179\n",
      "1 2 31 0.973109297129535\n",
      "1 2 32 0.9672412375697163\n",
      "1 2 33 0.9679650052614192\n",
      "1 2 34 0.9675366278571523\n",
      "1 2 35 0.968614054960474\n",
      "1 2 36 0.9693250222882539\n",
      "1 2 37 0.9677423907379595\n",
      "1 2 38 0.9679528749744347\n",
      "1 2 39 0.9664321121169539\n",
      "1 2 40 0.9650740524479061\n",
      "1 2 41 0.9678528182548392\n",
      "1 2 42 0.9707174492764892\n",
      "1 2 43 0.9677783274000066\n",
      "1 2 44 0.9696760573826758\n",
      "1 2 45 0.9626147958192298\n",
      "1 2 46 0.966476774159716\n",
      "1 2 47 0.9629103435795969\n",
      "1 2 48 0.9696588406691162\n",
      "1 2 49 0.9699868140899363\n",
      "1 2 50 0.9736281564543197\n",
      "1 2 51 0.9717834212569219\n",
      "1 2 52 0.97059785879458\n",
      "1 2 53 0.9634577343759676\n",
      "1 2 54 0.9653942893566196\n",
      "1 2 55 0.9682315768723041\n",
      "1 2 56 0.9645626169465219\n",
      "1 2 57 0.9672436638816543\n",
      "1 2 58 0.9687636436100818\n",
      "1 2 59 0.9706340091491068\n",
      "1 2 60 0.9645199308056448\n",
      "1 2 61 0.9702723330104714\n",
      "1 2 62 0.966252976689328\n",
      "1 2 63 0.9629861717167861\n",
      "1 2 64 0.9612384486124576\n",
      "1 2 65 0.9618699930143835\n",
      "1 2 66 0.9670686127136771\n",
      "1 2 67 0.9694251173008018\n",
      "1 2 68 0.9679316012330321\n",
      "1 2 69 0.9643013909520003\n",
      "1 2 70 0.9626187446877141\n",
      "1 2 71 0.9645240324026957\n",
      "1 2 72 0.9671140041601904\n",
      "1 2 73 0.9681068031119683\n",
      "1 2 74 0.962685437593213\n",
      "1 2 75 0.966323827739437\n",
      "1 2 76 0.9642014324635179\n",
      "1 2 77 0.968744866336348\n",
      "1 2 78 0.970979792974853\n",
      "1 2 79 0.9680777322941858\n",
      "1 2 80 0.9661790061913607\n",
      "1 2 81 0.9642790325175814\n",
      "1 2 82 0.9657653536145335\n",
      "1 2 83 0.9699248876470228\n",
      "1 2 84 0.9643753035581468\n",
      "1 2 85 0.9695551570241361\n",
      "1 2 86 0.9675267573803645\n",
      "1 2 87 0.9694755874886826\n",
      "1 2 88 0.9676506759783806\n",
      "1 2 89 0.9706708938742192\n",
      "1 2 90 0.966080890850098\n",
      "1 2 91 0.9696948863227425\n",
      "1 2 92 0.9699237668079721\n",
      "1 2 93 0.9591869303178182\n",
      "1 2 94 0.9704014745150653\n",
      "1 2 95 0.9647251170360375\n",
      "1 2 96 0.9695794923095341\n",
      "1 2 97 0.9737843215863086\n",
      "1 2 98 0.9656316662377913\n",
      "1 2 99 0.9683707546631598\n",
      "1 2 100 0.9638056715295995\n",
      "1 2 0.9663085802824053 0.0070252356546967685\n",
      "1 3 1 0.9504779251885335\n",
      "1 3 2 0.9256125523828062\n",
      "1 3 3 0.9314554560321466\n",
      "1 3 4 0.921466503065909\n",
      "1 3 5 0.9173564442308819\n",
      "1 3 6 0.937648635479583\n",
      "1 3 7 0.9160310127141013\n",
      "1 3 8 0.9469270968977367\n",
      "1 3 9 0.9122107246016227\n",
      "1 3 10 0.9436670722539839\n",
      "1 3 11 0.9243864976320962\n",
      "1 3 12 0.9426983248612528\n",
      "1 3 13 0.9387798661841175\n",
      "1 3 14 0.9151548590007232\n",
      "1 3 15 0.9276600184381665\n",
      "1 3 16 0.9338878870202838\n",
      "1 3 17 0.9250146440015728\n",
      "1 3 18 0.9051429899468568\n",
      "1 3 19 0.9216033714203352\n",
      "1 3 20 0.9342321749897895\n",
      "1 3 21 0.9347271463231498\n",
      "1 3 22 0.934424086077321\n",
      "1 3 23 0.9325749336646438\n",
      "1 3 24 0.9152539480158257\n",
      "1 3 25 0.9405161107378728\n",
      "1 3 26 0.9322313673085315\n",
      "1 3 27 0.9401858156153111\n",
      "1 3 28 0.9215047663577208\n",
      "1 3 29 0.9300088673404271\n",
      "1 3 30 0.9467344488714257\n",
      "1 3 31 0.9436670724795513\n",
      "1 3 32 0.9507160230040481\n",
      "1 3 33 0.9317278251429905\n",
      "1 3 34 0.9381057826801859\n",
      "1 3 35 0.9326472422262779\n",
      "1 3 36 0.9257079808133561\n",
      "1 3 37 0.9285527310311058\n",
      "1 3 38 0.9248190449346404\n",
      "1 3 39 0.9186960891632289\n",
      "1 3 40 0.9188368146544227\n",
      "1 3 41 0.8389446290559456\n",
      "1 3 42 0.935508330683521\n",
      "1 3 43 0.9179695458354442\n",
      "1 3 44 0.9258350463185065\n",
      "1 3 45 0.9360626630434278\n",
      "1 3 46 0.9079206824203666\n",
      "1 3 47 0.9329342274972535\n",
      "1 3 48 0.9278193620830241\n",
      "1 3 49 0.9262940502357319\n",
      "1 3 50 0.9335984024389088\n",
      "1 3 51 0.9199269535520982\n",
      "1 3 52 0.919557829346831\n",
      "1 3 53 0.9332402988921308\n",
      "1 3 54 0.9189570814970014\n",
      "1 3 55 0.933059390222582\n",
      "1 3 56 0.8909049129402684\n",
      "1 3 57 0.9286053027491523\n",
      "1 3 58 0.9127549090912359\n",
      "1 3 59 0.9013980192211704\n",
      "1 3 60 0.9263798624780987\n",
      "1 3 61 0.9251930636563459\n",
      "1 3 62 0.9282226909754394\n",
      "1 3 63 0.941382450819142\n",
      "1 3 64 0.9358321714924905\n",
      "1 3 65 0.9315420968137111\n",
      "1 3 66 0.9245994302761864\n",
      "1 3 67 0.9198762883328352\n",
      "1 3 68 0.9319974454940382\n",
      "1 3 69 0.9483971829284024\n",
      "1 3 70 0.9094761132533711\n",
      "1 3 71 0.9227395791175473\n",
      "1 3 72 0.9202629371854552\n",
      "1 3 73 0.9276262117578704\n",
      "1 3 74 0.9355217025788541\n",
      "1 3 75 0.9308066245951478\n",
      "1 3 76 0.9169059131028165\n",
      "1 3 77 0.9116282279116198\n",
      "1 3 78 0.9253229144765246\n",
      "1 3 79 0.9189655191388497\n",
      "1 3 80 0.9208027198890212\n",
      "1 3 81 0.9284193525822994\n",
      "1 3 82 0.9184911219281375\n",
      "1 3 83 0.9102857417615136\n",
      "1 3 84 0.9152351695862704\n",
      "1 3 85 0.9356403170948699\n",
      "1 3 86 0.9131000593746256\n",
      "1 3 87 0.9262795920454788\n",
      "1 3 88 0.9179842752882491\n",
      "1 3 89 0.9121267899210999\n",
      "1 3 90 0.9138989167844477\n",
      "1 3 91 0.9140227487716339\n",
      "1 3 92 0.9377324433353715\n",
      "1 3 93 0.895852106422516\n",
      "1 3 94 0.9444441895297261\n",
      "1 3 95 0.9334678016976936\n",
      "1 3 96 0.8983798343557987\n",
      "1 3 97 0.9106877630337903\n",
      "1 3 98 0.8892620209666393\n",
      "1 3 99 0.9554342058853982\n",
      "1 3 100 0.9317797713478693\n",
      "1 3 0.9251034915789633 0.015374780587429497\n",
      "2 1 1 0.8347240709493599\n",
      "2 1 2 0.753334652669758\n",
      "2 1 3 0.8014210038883487\n",
      "2 1 4 0.49909556629181856\n",
      "2 1 5 0.761689128403379\n",
      "2 1 6 0.8513889672385305\n",
      "2 1 7 0.771651700333372\n",
      "2 1 8 0.7815764654727154\n",
      "2 1 9 0.5051989640240573\n",
      "2 1 10 0.823753702921157\n",
      "2 1 11 0.7874187098101463\n",
      "2 1 12 0.5363345436445094\n",
      "2 1 13 0.5729360550796462\n",
      "2 1 14 0.8187281639917848\n",
      "2 1 15 0.8044200418784351\n",
      "2 1 16 0.7330345926130735\n",
      "2 1 17 0.8297832243120835\n",
      "2 1 18 0.5262810015915197\n",
      "2 1 19 0.8313739823721138\n",
      "2 1 20 0.758839725312785\n",
      "2 1 21 0.7310812620629938\n",
      "2 1 22 0.7403783836602909\n",
      "2 1 23 0.8275651963246609\n",
      "2 1 24 0.8176533131886501\n",
      "2 1 25 0.5030400366449943\n",
      "2 1 26 0.8486611429952209\n",
      "2 1 27 0.5137266674862829\n",
      "2 1 28 0.7872882361220828\n",
      "2 1 29 0.48696891242992024\n",
      "2 1 30 0.8233245980460236\n",
      "2 1 31 0.7936091725359121\n",
      "2 1 32 0.7742046866056731\n",
      "2 1 33 0.5512878254934049\n",
      "2 1 34 0.7897573499614878\n",
      "2 1 35 0.8381132386766053\n",
      "2 1 36 0.7711598460901371\n",
      "2 1 37 0.8271950609728385\n",
      "2 1 38 0.7627048865269535\n",
      "2 1 39 0.7788891397522053\n",
      "2 1 40 0.49758485416516457\n",
      "2 1 41 0.8401037886733497\n",
      "2 1 42 0.49284678092372286\n",
      "2 1 43 0.7949098456745517\n",
      "2 1 44 0.6089625663503522\n",
      "2 1 45 0.4501776134789837\n",
      "2 1 46 0.6430021106195172\n",
      "2 1 47 0.7897595272119867\n",
      "2 1 48 0.8256572565418644\n",
      "2 1 49 0.6151535492603555\n",
      "2 1 50 0.4771882579414009\n",
      "2 1 51 0.5373219100938833\n",
      "2 1 52 0.5155214156132735\n",
      "2 1 53 0.48682353566954073\n",
      "2 1 54 0.8341363575607463\n",
      "2 1 55 0.8319104149857806\n",
      "2 1 56 0.8007633012249731\n",
      "2 1 57 0.47699135946623267\n",
      "2 1 58 0.5004278403265163\n",
      "2 1 59 0.8310286227560234\n",
      "2 1 60 0.8033300441928676\n",
      "2 1 61 0.8307880875547677\n",
      "2 1 62 0.7851215749533283\n",
      "2 1 63 0.8260693255153353\n",
      "2 1 64 0.6773973979233784\n",
      "2 1 65 0.6093163092103484\n",
      "2 1 66 0.7218365475675342\n",
      "2 1 67 0.48695216913733425\n",
      "2 1 68 0.729822404769282\n",
      "2 1 69 0.8160327056161261\n",
      "2 1 70 0.4833061926806086\n",
      "2 1 71 0.3578385033811923\n",
      "2 1 72 0.8058836254001787\n",
      "2 1 73 0.4840944952739052\n",
      "2 1 74 0.5215616826438663\n",
      "2 1 75 0.7360990208990417\n",
      "2 1 76 0.8123959324066782\n",
      "2 1 77 0.5543442146492658\n",
      "2 1 78 0.7690880223582786\n",
      "2 1 79 0.8346285678863009\n",
      "2 1 80 0.8113498470860772\n",
      "2 1 81 0.7922718732737895\n",
      "2 1 82 0.5428031136797832\n",
      "2 1 83 0.8497833311247917\n",
      "2 1 84 0.7530115732655477\n",
      "2 1 85 0.5489378593227925\n",
      "2 1 86 0.5155633978700008\n",
      "2 1 87 0.7765846467993679\n",
      "2 1 88 0.7750211839511167\n",
      "2 1 89 0.8315330455495897\n",
      "2 1 90 0.8054992682558554\n",
      "2 1 91 0.738993962217212\n",
      "2 1 92 0.7836372633894949\n",
      "2 1 93 0.8169224395046997\n",
      "2 1 94 0.5335071895936527\n",
      "2 1 95 0.569560000961647\n",
      "2 1 96 0.8218953912403858\n",
      "2 1 97 0.8450902425478897\n",
      "2 1 98 0.7504143742578582\n",
      "2 1 99 0.7383876111237755\n",
      "2 1 100 0.771504316841315\n",
      "2 1 0.7041804288278939 0.1354287547368648\n",
      "2 2 1 0.7856613290541763\n",
      "2 2 2 0.8433588478330402\n",
      "2 2 3 0.9236028170951712\n",
      "2 2 4 0.7301893811954856\n",
      "2 2 5 0.8329061132541893\n",
      "2 2 6 0.7335487256179827\n",
      "2 2 7 0.9070434114938742\n",
      "2 2 8 0.7670735405881006\n",
      "2 2 9 0.8006172351594792\n",
      "2 2 10 0.8409855069209987\n",
      "2 2 11 0.9275670361509942\n",
      "2 2 12 0.8027988855240653\n",
      "2 2 13 0.816357662724118\n",
      "2 2 14 0.7355154029436846\n",
      "2 2 15 0.9043788577270183\n",
      "2 2 16 0.922434181791499\n",
      "2 2 17 0.914128849861103\n",
      "2 2 18 0.9209192455574216\n",
      "2 2 19 0.898306560789334\n",
      "2 2 20 0.9254641594592489\n",
      "2 2 21 0.8209052344395406\n",
      "2 2 22 0.9367806432310756\n",
      "2 2 23 0.794277886464467\n",
      "2 2 24 0.9358572092240985\n",
      "2 2 25 0.9245032604211597\n",
      "2 2 26 0.9336180379896032\n",
      "2 2 27 0.9232425444491359\n",
      "2 2 28 0.7910549297843714\n",
      "2 2 29 0.7975406195759045\n",
      "2 2 30 0.7851418651187214\n",
      "2 2 31 0.6615398687301831\n",
      "2 2 32 0.8163305159551784\n",
      "2 2 33 0.914313712203404\n",
      "2 2 34 0.836316746770774\n",
      "2 2 35 0.7780882916609931\n",
      "2 2 36 0.9209321530933671\n",
      "2 2 37 0.9034902448799529\n",
      "2 2 38 0.8353533490696043\n",
      "2 2 39 0.7733357028546286\n",
      "2 2 40 0.8964691755136109\n",
      "2 2 41 0.9101677095776786\n",
      "2 2 42 0.822564645015102\n",
      "2 2 43 0.9289424746291074\n",
      "2 2 44 0.9066727209300937\n",
      "2 2 45 0.930300224279426\n",
      "2 2 46 0.8997466619642639\n",
      "2 2 47 0.7938241238820437\n",
      "2 2 48 0.9289097459661529\n",
      "2 2 49 0.7958787905686582\n",
      "2 2 50 0.9047124791920149\n",
      "2 2 51 0.7993159594194308\n",
      "2 2 52 0.7429027618929782\n",
      "2 2 53 0.9441186089509368\n",
      "2 2 54 0.8052780713909544\n",
      "2 2 55 0.8347146605701722\n",
      "2 2 56 0.9280950064095039\n",
      "2 2 57 0.9091765728240564\n",
      "2 2 58 0.8392014426943466\n",
      "2 2 59 0.7900612663988023\n",
      "2 2 60 0.9194678196748368\n",
      "2 2 61 0.9310433214549677\n",
      "2 2 62 0.811782639230702\n",
      "2 2 63 0.9235666802956481\n",
      "2 2 64 0.8933608065335406\n",
      "2 2 65 0.9374237016448644\n",
      "2 2 66 0.796050567505827\n",
      "2 2 67 0.8250185192027016\n",
      "2 2 68 0.8249140099855793\n",
      "2 2 69 0.8128782808634756\n",
      "2 2 70 0.9004131833502381\n",
      "2 2 71 0.8248884278220343\n",
      "2 2 72 0.8187822972538854\n",
      "2 2 73 0.9038272660943041\n",
      "2 2 74 0.9158165269041303\n",
      "2 2 75 0.7900397991760318\n",
      "2 2 76 0.7841083548445703\n",
      "2 2 77 0.9132517800011535\n",
      "2 2 78 0.8766078666340568\n",
      "2 2 79 0.799069165522355\n",
      "2 2 80 0.7765651915910559\n",
      "2 2 81 0.8427279526233833\n",
      "2 2 82 0.8058119918519149\n",
      "2 2 83 0.7738977224359279\n",
      "2 2 84 0.9156511918433307\n",
      "2 2 85 0.9128723521667778\n",
      "2 2 86 0.7752299139971645\n",
      "2 2 87 0.9192849716403191\n",
      "2 2 88 0.9243876150038979\n",
      "2 2 89 0.7441767327227571\n",
      "2 2 90 0.8315540788066534\n",
      "2 2 91 0.7654786562642176\n",
      "2 2 92 0.8341817146223333\n",
      "2 2 93 0.8005221213084396\n",
      "2 2 94 0.7987506976591642\n",
      "2 2 95 0.9151004394696686\n",
      "2 2 96 0.9146498029325276\n",
      "2 2 97 0.7842057041676563\n",
      "2 2 98 0.9227911687546678\n",
      "2 2 99 0.7980111062645637\n",
      "2 2 100 0.8251679304602849\n",
      "2 2 0.8510986373930811 0.06533457927479093\n",
      "2 3 1 0.9305705820885675\n",
      "2 3 2 0.9078478218162699\n",
      "2 3 3 0.9221558940822823\n",
      "2 3 4 0.9230336442711935\n",
      "2 3 5 0.9051542160894006\n",
      "2 3 6 0.9291243604869893\n",
      "2 3 7 0.9290043773038399\n",
      "2 3 8 0.9193293333050031\n",
      "2 3 9 0.9275618507132866\n",
      "2 3 10 0.9224660014414794\n",
      "2 3 11 0.8212723100797858\n",
      "2 3 12 0.9160178915036592\n",
      "2 3 13 0.910640421962157\n",
      "2 3 14 0.8997268026512488\n",
      "2 3 15 0.9101890614940014\n",
      "2 3 16 0.930560147960466\n",
      "2 3 17 0.9260447227678885\n",
      "2 3 18 0.9318382232033776\n",
      "2 3 19 0.9184239309919734\n",
      "2 3 20 0.9273904740811267\n",
      "2 3 21 0.903565364462407\n",
      "2 3 22 0.9125729887588234\n",
      "2 3 23 0.912139459093216\n",
      "2 3 24 0.8870614192014826\n",
      "2 3 25 0.920037045275823\n",
      "2 3 26 0.9242816499170221\n",
      "2 3 27 0.9121087201061134\n",
      "2 3 28 0.9403435081566442\n",
      "2 3 29 0.9285782284345124\n",
      "2 3 30 0.9378683342783569\n",
      "2 3 31 0.9423747043181862\n",
      "2 3 32 0.881535081516419\n",
      "2 3 33 0.912039807271456\n",
      "2 3 34 0.570617465088113\n",
      "2 3 35 0.8933665863683047\n",
      "2 3 36 0.9359498941474887\n",
      "2 3 37 0.9146677287889545\n",
      "2 3 38 0.9195277165931665\n",
      "2 3 39 0.9127753705608046\n",
      "2 3 40 0.9043203638768046\n",
      "2 3 41 0.9042384473668296\n",
      "2 3 42 0.869499164957877\n",
      "2 3 43 0.928812192840567\n",
      "2 3 44 0.9287649745500561\n",
      "2 3 45 0.904753040158661\n",
      "2 3 46 0.9335584378501286\n",
      "2 3 47 0.9181778685375741\n",
      "2 3 48 0.8943032089710193\n",
      "2 3 49 0.921809041314057\n",
      "2 3 50 0.9195011441937452\n",
      "2 3 51 0.9187281717800599\n",
      "2 3 52 0.9375625956510932\n",
      "2 3 53 0.9198830827826341\n",
      "2 3 54 0.9246585770890557\n",
      "2 3 55 0.9074917603920571\n",
      "2 3 56 0.9311104331406552\n",
      "2 3 57 0.9123222726418286\n",
      "2 3 58 0.9010229239252684\n",
      "2 3 59 0.9165422223944327\n",
      "2 3 60 0.8998831504336767\n",
      "2 3 61 0.908118471799451\n",
      "2 3 62 0.9090203471335428\n",
      "2 3 63 0.9201852836597476\n",
      "2 3 64 0.9282980813382264\n",
      "2 3 65 0.9231121792487649\n",
      "2 3 66 0.9335003340117756\n",
      "2 3 67 0.9257311169405418\n",
      "2 3 68 0.9094051222301733\n",
      "2 3 69 0.9399594530755964\n",
      "2 3 70 0.9244752963343629\n",
      "2 3 71 0.9086937786095376\n",
      "2 3 72 0.9140896983784785\n",
      "2 3 73 0.9231569195359548\n",
      "2 3 74 0.9221134138509772\n",
      "2 3 75 0.9293831242476307\n",
      "2 3 76 0.9280236954353062\n",
      "2 3 77 0.9230963297552158\n",
      "2 3 78 0.9339878007380519\n",
      "2 3 79 0.9287143921898405\n",
      "2 3 80 0.8938613241380003\n",
      "2 3 81 0.8946378614952561\n",
      "2 3 82 0.9143111854907642\n",
      "2 3 83 0.8863171696494879\n",
      "2 3 84 0.9177483194352637\n",
      "2 3 85 0.9372819324110481\n",
      "2 3 86 0.9248538531584113\n",
      "2 3 87 0.8944008038958798\n",
      "2 3 88 0.9278066102426799\n",
      "2 3 89 0.9127684331581299\n",
      "2 3 90 0.9097762872046384\n",
      "2 3 91 0.9263560171896558\n",
      "2 3 92 0.912103842934801\n",
      "2 3 93 0.925563005228644\n",
      "2 3 94 0.9412787158649164\n",
      "2 3 95 0.889293659384972\n",
      "2 3 96 0.9203131486459226\n",
      "2 3 97 0.9132568724430942\n",
      "2 3 98 0.834006212599004\n",
      "2 3 99 0.917884049370775\n",
      "2 3 100 0.9087272175769098\n",
      "2 3 0.9120231757551082 0.0390895762124846\n",
      "3 1 1 0.7798927007303655\n",
      "3 1 2 0.676050597156241\n",
      "3 1 3 0.566607961122725\n",
      "3 1 4 0.6302121149748471\n",
      "3 1 5 0.7467885906229438\n",
      "3 1 6 0.6784943652088997\n",
      "3 1 7 0.7164814817585057\n",
      "3 1 8 0.7218438171099572\n",
      "3 1 9 0.650477233721952\n",
      "3 1 10 0.7020555206638472\n",
      "3 1 11 0.7389531352726229\n",
      "3 1 12 0.7448925773529956\n",
      "3 1 13 0.5059464166716816\n",
      "3 1 14 0.7364617081385292\n",
      "3 1 15 0.7690603249372906\n",
      "3 1 16 0.7599520446117841\n",
      "3 1 17 0.7271505520732868\n",
      "3 1 18 0.7169910902055137\n",
      "3 1 19 0.7781817188182506\n",
      "3 1 20 0.7571007710267027\n",
      "3 1 21 0.6799424802819077\n",
      "3 1 22 0.761912622273604\n",
      "3 1 23 0.5643863010598941\n",
      "3 1 24 0.6514805099122335\n",
      "3 1 25 0.7686345586387208\n",
      "3 1 26 0.7295727323442098\n",
      "3 1 27 0.702441261133172\n",
      "3 1 28 0.7722284113466781\n",
      "3 1 29 0.7205827266360976\n",
      "3 1 30 0.7081833547851423\n",
      "3 1 31 0.7505195366794203\n",
      "3 1 32 0.7619600404216548\n",
      "3 1 33 0.721557915915625\n",
      "3 1 34 0.7496118836269137\n",
      "3 1 35 0.5957453743504991\n",
      "3 1 36 0.7391456514404017\n",
      "3 1 37 0.5247586015775408\n",
      "3 1 38 0.7483816716928491\n",
      "3 1 39 0.7302848460935137\n",
      "3 1 40 0.6800018398149321\n",
      "3 1 41 0.7741546700164567\n",
      "3 1 42 0.745439851869256\n",
      "3 1 43 0.5694527815152891\n",
      "3 1 44 0.7514749080111323\n",
      "3 1 45 0.48719097296684427\n",
      "3 1 46 0.7359109349330801\n",
      "3 1 47 0.7630784021922148\n",
      "3 1 48 0.7280105466448986\n",
      "3 1 49 0.74057668665313\n",
      "3 1 50 0.7473758179210518\n",
      "3 1 51 0.7801220742670961\n",
      "3 1 52 0.7768917581501785\n",
      "3 1 53 0.6952491945290784\n",
      "3 1 54 0.6874140754149541\n",
      "3 1 55 0.76203343972852\n",
      "3 1 56 0.3934545675668205\n",
      "3 1 57 0.7104617342758092\n",
      "3 1 58 0.7524839448403781\n",
      "3 1 59 0.7515491932423539\n",
      "3 1 60 0.7703706222350557\n",
      "3 1 61 0.7579881575087087\n",
      "3 1 62 0.7308108614372258\n",
      "3 1 63 0.5790386059199223\n",
      "3 1 64 0.7529212962358093\n",
      "3 1 65 0.7561372791185603\n",
      "3 1 66 0.6950787360255617\n",
      "3 1 67 0.6846925867421616\n",
      "3 1 68 0.7760674314249282\n",
      "3 1 69 0.7569354807633917\n",
      "3 1 70 0.2826868416104097\n",
      "3 1 71 0.7093754690045665\n",
      "3 1 72 0.6364477719134496\n",
      "3 1 73 0.6769734227640775\n",
      "3 1 74 0.6748197114049579\n",
      "3 1 75 0.7681772284704418\n",
      "3 1 76 0.750184828795005\n",
      "3 1 77 0.7127386210283461\n",
      "3 1 78 0.7330416893117966\n",
      "3 1 79 0.7396660118550272\n",
      "3 1 80 0.4800840776851964\n",
      "3 1 81 0.7262544379104643\n",
      "3 1 82 0.7423058203121008\n",
      "3 1 83 0.5385019104933922\n",
      "3 1 84 0.74848514938902\n",
      "3 1 85 0.7588210011009694\n",
      "3 1 86 0.6984885380618743\n",
      "3 1 87 0.7463520738731876\n",
      "3 1 88 0.6956310818808168\n",
      "3 1 89 0.768900104264889\n",
      "3 1 90 0.7344562791109694\n",
      "3 1 91 0.7558966559095321\n",
      "3 1 92 0.6977635553407412\n",
      "3 1 93 0.7254987081216514\n",
      "3 1 94 0.7751393814647662\n",
      "3 1 95 0.8105319086272853\n",
      "3 1 96 0.664394780060642\n",
      "3 1 97 0.8007380156935098\n",
      "3 1 98 0.768974069840711\n",
      "3 1 99 0.7328614517448738\n",
      "3 1 100 0.6571960875797801\n",
      "3 1 0.7048868033897826 0.08632570773278522\n",
      "3 2 1 0.911602002542621\n",
      "3 2 2 0.9195341397075428\n",
      "3 2 3 0.9019598683981801\n",
      "3 2 4 0.940201456384484\n",
      "3 2 5 0.909490474454615\n",
      "3 2 6 0.9112041047294753\n",
      "3 2 7 0.8263102291368887\n",
      "3 2 8 0.9067873169288933\n",
      "3 2 9 0.9307101163049917\n",
      "3 2 10 0.9333918535456505\n",
      "3 2 11 0.9347706060864469\n",
      "3 2 12 0.7709613172372551\n",
      "3 2 13 0.8785846588206505\n",
      "3 2 14 0.7226256323384203\n",
      "3 2 15 0.9229347441548341\n",
      "3 2 16 0.9245556659354169\n",
      "3 2 17 0.9265762384985063\n",
      "3 2 18 0.931797269049771\n",
      "3 2 19 0.9286735500640823\n",
      "3 2 20 0.8007762416290305\n",
      "3 2 21 0.9468259914309668\n",
      "3 2 22 0.607385274040232\n",
      "3 2 23 0.7349782649608444\n",
      "3 2 24 0.9223933445737161\n",
      "3 2 25 0.9191759631120974\n",
      "3 2 26 0.9244350308167913\n",
      "3 2 27 0.7628524517855199\n",
      "3 2 28 0.9239990968511486\n",
      "3 2 29 0.8878985077500375\n",
      "3 2 30 0.8945093136535991\n",
      "3 2 31 0.9031186637703995\n",
      "3 2 32 0.9128318689793725\n",
      "3 2 33 0.9068739140422574\n",
      "3 2 34 0.9211811536895433\n",
      "3 2 35 0.9180296221129036\n",
      "3 2 36 0.9057111334614742\n",
      "3 2 37 0.8597826377308161\n",
      "3 2 38 0.8229785245802909\n",
      "3 2 39 0.9180609223264827\n",
      "3 2 40 0.9152681601827655\n",
      "3 2 41 0.8510112457098887\n",
      "3 2 42 0.9084231927768022\n",
      "3 2 43 0.920154888941887\n",
      "3 2 44 0.9208814172529017\n",
      "3 2 45 0.9208006638181263\n",
      "3 2 46 0.9308953342440922\n",
      "3 2 47 0.7479564558374057\n",
      "3 2 48 0.9029249674803734\n",
      "3 2 49 0.9040531349934665\n",
      "3 2 50 0.9378230008118617\n",
      "3 2 51 0.5965666353452288\n",
      "3 2 52 0.9354104670606173\n",
      "3 2 53 0.9180045289650739\n",
      "3 2 54 0.7154445149201027\n",
      "3 2 55 0.8461044649041833\n",
      "3 2 56 0.9194781877615514\n",
      "3 2 57 0.8733356280757597\n",
      "3 2 58 0.9234908159031272\n",
      "3 2 59 0.893595976773762\n",
      "3 2 60 0.9124956167279247\n",
      "3 2 61 0.8955698489825776\n",
      "3 2 62 0.9253774387799822\n",
      "3 2 63 0.9363154719003814\n",
      "3 2 64 0.7405809037914579\n",
      "3 2 65 0.9169462504938558\n",
      "3 2 66 0.8985646198785338\n",
      "3 2 67 0.8844762543803715\n",
      "3 2 68 0.9400823263684616\n",
      "3 2 69 0.925808870924716\n",
      "3 2 70 0.9101911474080544\n",
      "3 2 71 0.9220889095561056\n",
      "3 2 72 0.9164538765157986\n",
      "3 2 73 0.9089779572620074\n",
      "3 2 74 0.8612368683397152\n",
      "3 2 75 0.9022672933776059\n",
      "3 2 76 0.9255825190522734\n",
      "3 2 77 0.7832139519624406\n",
      "3 2 78 0.9225122068326045\n",
      "3 2 79 0.8290512336089513\n",
      "3 2 80 0.9139360578888546\n",
      "3 2 81 0.9131194128271215\n",
      "3 2 82 0.9137357713979564\n",
      "3 2 83 0.9326338863512245\n",
      "3 2 84 0.8962823598485169\n",
      "3 2 85 0.7741993273096707\n",
      "3 2 86 0.9081856927225641\n",
      "3 2 87 0.8214300948871025\n",
      "3 2 88 0.8994417341828146\n",
      "3 2 89 0.9192719467427897\n",
      "3 2 90 0.5683305215537029\n",
      "3 2 91 0.9233503463017976\n",
      "3 2 92 0.9219956393955252\n",
      "3 2 93 0.8426251165606612\n",
      "3 2 94 0.8811436632573012\n",
      "3 2 95 0.8463995342925897\n",
      "3 2 96 0.9268762923841927\n",
      "3 2 97 0.9109945646931255\n",
      "3 2 98 0.9061171268845015\n",
      "3 2 99 0.9157407992124605\n",
      "3 2 100 0.6676740422831882\n",
      "3 2 0.8796937034449668 0.07668774865498043\n",
      "3 3 1 0.8798378003959553\n",
      "3 3 2 0.8276769450791852\n",
      "3 3 3 0.8798293258545011\n",
      "3 3 4 0.8354813604394838\n",
      "3 3 5 0.8525022019939957\n",
      "3 3 6 0.8449590093627463\n",
      "3 3 7 0.8234180420200877\n",
      "3 3 8 0.8685285115495969\n",
      "3 3 9 0.742581316937544\n",
      "3 3 10 0.8164448872375106\n",
      "3 3 11 0.886298263468839\n",
      "3 3 12 0.8605335096866769\n",
      "3 3 13 0.8725304774864076\n",
      "3 3 14 0.8626518542293196\n",
      "3 3 15 0.9036210826404751\n",
      "3 3 16 0.8489191742998962\n",
      "3 3 17 0.7514713947440008\n",
      "3 3 18 0.864153893073351\n",
      "3 3 19 0.8101638289955241\n",
      "3 3 20 0.8106034033455531\n",
      "3 3 21 0.8689320070360274\n",
      "3 3 22 0.8725237863830447\n",
      "3 3 23 0.8300703878217697\n",
      "3 3 24 0.8568479563628361\n",
      "3 3 25 0.887202925141166\n",
      "3 3 26 0.8916061446167447\n",
      "3 3 27 0.7914507494051279\n",
      "3 3 28 0.8614283730162065\n",
      "3 3 29 0.8122323038683253\n",
      "3 3 30 0.8777299224501746\n",
      "3 3 31 0.8567931910225457\n",
      "3 3 32 0.8931899273479833\n",
      "3 3 33 0.8690662312200492\n",
      "3 3 34 0.7972754932842516\n",
      "3 3 35 0.8851148571245989\n",
      "3 3 36 0.8266437206550279\n",
      "3 3 37 0.8737817068696936\n",
      "3 3 38 0.8415804957744603\n",
      "3 3 39 0.8904990671065642\n",
      "3 3 40 0.5594561247612839\n",
      "3 3 41 0.8781103966318272\n",
      "3 3 42 0.6286280328390927\n",
      "3 3 43 0.8728996156149952\n",
      "3 3 44 0.8271368359376446\n",
      "3 3 45 0.6730481214929648\n",
      "3 3 46 0.8694354095977374\n",
      "3 3 47 0.8625504002423913\n",
      "3 3 48 0.7018451388137594\n",
      "3 3 49 0.847290835282356\n",
      "3 3 50 0.8604678408611031\n",
      "3 3 51 0.6279193610002417\n",
      "3 3 52 0.8881399027868585\n",
      "3 3 53 0.8524212661688552\n",
      "3 3 54 0.7281818906946074\n",
      "3 3 55 0.8592788217398265\n",
      "3 3 56 0.8752590172901555\n",
      "3 3 57 0.8117836167381549\n",
      "3 3 58 0.8546491190351965\n",
      "3 3 59 0.8084892409231784\n",
      "3 3 60 0.904667278793688\n",
      "3 3 61 0.7951987458450502\n",
      "3 3 62 0.818388249866713\n",
      "3 3 63 0.866105489802633\n",
      "3 3 64 0.7620875281278647\n",
      "3 3 65 0.8102127451965376\n",
      "3 3 66 0.8582300890647336\n",
      "3 3 67 0.8577630658254444\n",
      "3 3 68 0.8608996324447463\n",
      "3 3 69 0.9022216166877265\n",
      "3 3 70 0.8801177804565236\n",
      "3 3 71 0.8442363989787883\n",
      "3 3 72 0.8603665385649889\n",
      "3 3 73 0.8596523694871945\n",
      "3 3 74 0.7832886796105519\n",
      "3 3 75 0.8652171382962562\n",
      "3 3 76 0.8565337509054299\n",
      "3 3 77 0.7629221013460442\n",
      "3 3 78 0.8967276676293832\n",
      "3 3 79 0.8444790457636334\n",
      "3 3 80 0.8320575134449282\n",
      "3 3 81 0.8947600369149601\n",
      "3 3 82 0.8819532479579599\n",
      "3 3 83 0.8790280118794431\n",
      "3 3 84 0.8694397142263485\n",
      "3 3 85 0.8852037704130533\n",
      "3 3 86 0.8274492973328307\n",
      "3 3 87 0.8608914510057118\n",
      "3 3 88 0.8565064807260397\n",
      "3 3 89 0.8936200841709977\n",
      "3 3 90 0.8569760619150951\n",
      "3 3 91 0.8875921642930324\n",
      "3 3 92 0.8814157863105636\n",
      "3 3 93 0.889817116375705\n",
      "3 3 94 0.7586671179535068\n",
      "3 3 95 0.8783808256206619\n",
      "3 3 96 0.8872972409597337\n",
      "3 3 97 0.8879954411569195\n",
      "3 3 98 0.8839725280447859\n",
      "3 3 99 0.8435079725587183\n",
      "3 3 100 0.8863608163223331\n",
      "3 3 0.8412537511007673 0.06055803318654885\n"
     ]
    }
   ],
   "source": [
    "n=2000\n",
    "for model1 in range(1,4):\n",
    "    for model2 in range(1,4):\n",
    "        dcor_list=[]\n",
    "        for t in range(1,101):\n",
    "            x_train=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/x_train_\" + str(t) + \".csv\")\n",
    "            x_train=x_train.drop('Unnamed: 0', axis=1)\n",
    "            y_train=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_train_\" + str(t) + \".csv\")\n",
    "            y_train=y_train.drop('Unnamed: 0', axis=1)\n",
    "            x_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/x_test_\" + str(t) + \".csv\")\n",
    "            x_test=x_test.drop('Unnamed: 0', axis=1)\n",
    "            y_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_test_\" + str(t) + \".csv\")\n",
    "            y_test=y_test.drop('Unnamed: 0', axis=1)\n",
    "            z_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/z_test_\" + str(t) + \".csv\")\n",
    "            z_test=z_test.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "\n",
    "            n=x_train.shape[0]\n",
    "            p=x_train.shape[1]\n",
    "            res_d=1\n",
    "            m=2\n",
    "            x_train = torch.tensor(x_train.values).to(torch.float)\n",
    "            y_train = torch.tensor(y_train.values).to(torch.float)\n",
    "            x_test = torch.tensor(x_test.values).to(torch.float)\n",
    "            y_test = torch.tensor(y_test.values).to(torch.float)\n",
    "            mse_loss = nn.MSELoss()\n",
    "            \n",
    "            class nn_dr_reg_model(nn.Module):\n",
    "                def __init__(self, input_features, output_features, dim_red_features, hidden_units_d, hidden_units_e, dim_red_layers, ens_reg_layers):\n",
    "                    super().__init__()\n",
    "                    model_dim_red=[]\n",
    "                    model_dim_red.append(nn.Linear(in_features=input_features, \n",
    "                                                out_features=hidden_units_d))\n",
    "                    model_dim_red.append(nn.ReLU())\n",
    "                    for i in range(1,dim_red_layers):\n",
    "                        model_dim_red.append(nn.Linear(in_features=hidden_units_d, \n",
    "                                                    out_features=hidden_units_d))\n",
    "                        model_dim_red.append(nn.ReLU())\n",
    "                    model_dim_red.append(nn.Linear(in_features=hidden_units_d, \n",
    "                                                out_features=dim_red_features))\n",
    "                    self.dim_red_layer_stack = nn.Sequential(*model_dim_red)\n",
    "\n",
    "                    model_ens_reg=[]\n",
    "                    model_ens_reg.append(nn.Linear(in_features=dim_red_features, out_features=hidden_units_e))\n",
    "                    model_ens_reg.append(nn.ReLU())\n",
    "                    for i in range(1,ens_reg_layers):\n",
    "                        model_ens_reg.append(nn.Linear(in_features=hidden_units_e, out_features=hidden_units_e))\n",
    "                        model_ens_reg.append(nn.ReLU())\n",
    "                    model_ens_reg.append(nn.Linear(in_features=hidden_units_e, out_features=output_features))\n",
    "                    self.ens_reg_layer_stack = nn.Sequential(*model_ens_reg)\n",
    "                \n",
    "                def forward(self, x):\n",
    "                    suff_predictor = self.dim_red_layer_stack(x)\n",
    "                    ens_output = self.ens_reg_layer_stack(suff_predictor)\n",
    "                    return ens_output, suff_predictor\n",
    "                    \n",
    "\n",
    "            model_nn = nn_dr_reg_model(input_features=p, \n",
    "                                    output_features=1, \n",
    "                                    dim_red_features=res_d, \n",
    "                                    hidden_units_d=300,\n",
    "                                    hidden_units_e=300,\n",
    "                                    dim_red_layers=4, \n",
    "                                    ens_reg_layers=4\n",
    "                                    ).to(device)\n",
    "            model_nn\n",
    "            optimizer = torch.optim.Adam(model_nn.parameters(), \n",
    "                                        lr=0.001)\n",
    "            epochs = 200\n",
    "            x_train, y_train = x_train.to(device), y_train.to(device)\n",
    "            x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "            for epoch in range(epochs):\n",
    "                ### Training\n",
    "                model_nn.train()\n",
    "                y_pred_train, y_suff_train = model_nn(x_train) \n",
    "                loss = mse_loss(y_pred_train, y_train) \n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                ### Testing\n",
    "                model_nn.eval()\n",
    "                y_pred_test, y_suff_test = model_nn(x_test)\n",
    "                loss_test = mse_loss(y_pred_test, y_test) \n",
    "                #dcor_test = dcor.distance_correlation(y_suff_test.detach().numpy(),z_test)\n",
    "\n",
    "                if epoch % 25 == 0:\n",
    "                    #print(f\"Epoch: {epoch} | Loss: {loss:.5f} | Test Loss: {loss_test:.5f} | Dcor: {dcor_test:.5f}\")\n",
    "                    now = datetime.now()\n",
    "                    current_time = now.strftime(\"%H:%M:%S\")\n",
    "                    #print(\"Current Time =\", current_time)\n",
    "            model_nn.eval()\n",
    "            with torch.inference_mode():\n",
    "                y_pred_test, y_suff_test = model_nn(x_test)\n",
    "            y_suff_test=y_suff_test.numpy()\n",
    "            dcor_current=dcor.distance_correlation(np.float64(y_suff_test),np.float64(z_test.to_numpy()))\n",
    "            dcor_list.append(dcor_current)\n",
    "            print(model1, model2, t, dcor_current)\n",
    "        dcor_list_df=pd.DataFrame(dcor_list)\n",
    "        dcor_list_df.to_csv(\"./results-BENN/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \".csv\")\n",
    "        print(model1, model2, np.mean(dcor_list), np.std(dcor_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_conda",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
