{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de454e8-5aa6-49bc-bc91-04b6e707efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import errno\n",
    "import torch.utils.data\n",
    "from model import Net, Resize\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn import svm\n",
    "#from sklearn import metrics\n",
    "#from process_data import preprocess_data\n",
    "from train_stonet import train_stonet\n",
    "from model import DNN\n",
    "import argparse\n",
    "import time\n",
    "from time import process_time\n",
    "import math\n",
    "from tools import *\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#import dcor\n",
    "\n",
    "\n",
    "from pytorch_lightning import seed_everything\n",
    "seed_everything(42, workers=True)\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1a0f42-5258-4733-87a9-3b0c22fee3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25, 2, 1]\n",
      "4 1 1000 1\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Running StoNet\")\n",
    "parser.add_argument('--seed', default=1, type = int, help = 'set seed')\n",
    "parser.add_argument('--data_path', default=\"./data/\", type=str, help='folder name for loading data')\n",
    "parser.add_argument('--base_path', default='./result/', type = str, help = 'base path for saving result')\n",
    "parser.add_argument('--model_path', default='./', type=str, help='folder name for saving model')\n",
    "parser.add_argument('--data_index', default=0, type=int)\n",
    "parser.add_argument('--data_name', default='waveform', type=str, help='the name of the dataset')\n",
    "parser.add_argument('--device', default='cpu', type=str, help='the device used to run the experiment')\n",
    "parser.add_argument('--model1', default=4, type = int, help = 'model1')\n",
    "parser.add_argument('--model2', default=1, type = int, help = 'model2')\n",
    "parser.add_argument('--n', default=1000, type = int, help = 'n')\n",
    "parser.add_argument('--r', default=25, type = int, help = 'r')\n",
    "parser.add_argument('--t', default=1, type = int, help = 't')\n",
    "parser.add_argument('--d', default=2, type = int, help = 'd')\n",
    "\n",
    "\n",
    "# Training Setting\n",
    "parser.add_argument('--num_epochs', default=60, type=int, help='total number of training epochs')\n",
    "parser.add_argument('--momentum', default=0.9, type=float, help='momentum in SGD')\n",
    "parser.add_argument('--weight_decay', default=0, type=float, help='weight decay in SGD')\n",
    "parser.add_argument('--batch_size', default=64, type=int, help='batch size for training')\n",
    "parser.add_argument('--MH_step', default=25, type=int, help='SGLD step for imputation')\n",
    "parser.add_argument('--sigma_list', default=[1e-2, 1e-4], nargs='+', type=float, help='list of sigma for stonet')\n",
    "parser.add_argument('--alpha', default=0.1, type=float, help='alpha for HMC')\n",
    "parser.add_argument('--proposal_lr', default=[7e-4, 7e-4], nargs='+', type=float, help='learning rate for training stonet')\n",
    "parser.add_argument('--step_size', default=0.01, type=float, help='learning rate for the optimizer')\n",
    "parser.add_argument('--net_architecture', default=[10, 1, 1], nargs='+', type=int, help='the architecture of the global stonet')\n",
    "parser.add_argument('--cross_validate_index', default=1, type=int, help='specify which fold of 5 fold cross validation')\n",
    "parser.add_argument('--regression_flag', default=1, type=int, help='1 for regression and 0 for classification')\n",
    "parser.add_argument('--confidence_interval_flag', default=False, type=int, help='whether to store result to compute confidence interval')\n",
    "parser.add_argument('--model_type', default='Logistic', type=str, help=\"the model to fit on the dimension reduced data including: 'Logistic', 'SVM', 'DNN'\")\n",
    "parser.add_argument('--activation', default='Tanh', type=str, help=\"the type of the activation function used in StoNet including: 'ReLU', 'Tanh'\")\n",
    "\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = args.device\n",
    "seed = args.seed\n",
    "data_name = args.data_name\n",
    "base_path = args.base_path\n",
    "model_path = args.model_path\n",
    "cross_validate_index = args.cross_validate_index\n",
    "regression_flag = args.regression_flag\n",
    "num_epochs = args.num_epochs\n",
    "model_path = args.model_path\n",
    "proposal_lr = args.proposal_lr\n",
    "step_size = args.step_size\n",
    "sigma_list = args.sigma_list\n",
    "alpha = args.alpha\n",
    "MH_step = args.MH_step\n",
    "momentum = args.momentum\n",
    "weight_decay = args.weight_decay\n",
    "batch_size = args.batch_size\n",
    "net_architecture = [args.r, args.d, 1]\n",
    "num_hidden = len(net_architecture) - 1\n",
    "model_type = args.model_type\n",
    "activation = args.activation\n",
    "print(net_architecture)\n",
    "\n",
    "model1 = args.model1\n",
    "model2 = args.model2\n",
    "n = args.n\n",
    "t = args.t\n",
    "print(model1, model2, n, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c127a1d-1698-42ff-90fc-ab792c3b4e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory=\"./results-StoNet-\" + str(args.r) + \"/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n)\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659d0026-416c-40a3-9d95-481110957100",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1_start = time.time() \n",
    "t2_start = process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11cf265-a3fd-4e4e-a791-678392dcd33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0\n",
      "train loss:  tensor(1.8950) ; train corr:  0.2802441178153188\n",
      "test loss:  tensor(1.8023) ; test corr:  0.15201739548375867\n",
      "epoch:  1\n",
      "train loss:  tensor(1.8600) ; train corr:  0.3576073486095219\n",
      "test loss:  tensor(2.0264) ; test corr:  0.1972059799654245\n",
      "epoch:  2\n",
      "train loss:  tensor(1.8289) ; train corr:  0.3577953298702642\n",
      "test loss:  tensor(1.7801) ; test corr:  0.19048736088555687\n",
      "epoch:  3\n",
      "train loss:  tensor(1.6981) ; train corr:  0.41004501932853954\n",
      "test loss:  tensor(1.9391) ; test corr:  0.18181431080712282\n",
      "epoch:  4\n",
      "train loss:  tensor(1.7498) ; train corr:  0.39776342455254454\n",
      "test loss:  tensor(1.7853) ; test corr:  0.14954255179846648\n",
      "epoch:  5\n",
      "train loss:  tensor(1.5139) ; train corr:  0.5216430495865273\n",
      "test loss:  tensor(1.7397) ; test corr:  0.3212815903785199\n",
      "epoch:  6\n",
      "train loss:  tensor(1.4188) ; train corr:  0.5478168276737017\n",
      "test loss:  tensor(1.7942) ; test corr:  0.2928939959856932\n",
      "epoch:  7\n",
      "train loss:  tensor(1.4811) ; train corr:  0.5375100006257546\n",
      "test loss:  tensor(1.8249) ; test corr:  0.25975497857782465\n",
      "epoch:  8\n",
      "train loss:  tensor(1.3615) ; train corr:  0.5673514144580358\n",
      "test loss:  tensor(1.8220) ; test corr:  0.2609632708608533\n",
      "epoch:  9\n",
      "train loss:  tensor(1.5579) ; train corr:  0.6077752189277831\n",
      "test loss:  tensor(2.0411) ; test corr:  0.2149631735940747\n",
      "epoch:  10\n",
      "train loss:  tensor(1.1747) ; train corr:  0.6446745004175909\n",
      "test loss:  tensor(1.8547) ; test corr:  0.312946945902226\n",
      "epoch:  11\n",
      "train loss:  tensor(1.2805) ; train corr:  0.6167120077612124\n",
      "test loss:  tensor(1.8899) ; test corr:  0.2935228963876389\n",
      "epoch:  12\n",
      "train loss:  tensor(1.2051) ; train corr:  0.6329508887126778\n",
      "test loss:  tensor(1.9058) ; test corr:  0.28342708571714054\n",
      "epoch:  13\n",
      "train loss:  tensor(1.0712) ; train corr:  0.6800547683990407\n",
      "test loss:  tensor(1.7957) ; test corr:  0.3287332499632069\n",
      "epoch:  14\n",
      "train loss:  tensor(1.1233) ; train corr:  0.6662930868788403\n",
      "test loss:  tensor(1.8747) ; test corr:  0.33217582073336493\n",
      "epoch:  15\n",
      "train loss:  tensor(1.0436) ; train corr:  0.6922115177096345\n",
      "test loss:  tensor(1.8319) ; test corr:  0.3224131828810209\n",
      "epoch:  16\n",
      "train loss:  tensor(1.0157) ; train corr:  0.7012557651957224\n",
      "test loss:  tensor(1.8959) ; test corr:  0.2911992912719521\n",
      "epoch:  17\n",
      "train loss:  tensor(1.0895) ; train corr:  0.7179134167913485\n",
      "test loss:  tensor(1.9464) ; test corr:  0.28694607106372577\n",
      "epoch:  18\n",
      "train loss:  tensor(1.1616) ; train corr:  0.6976645063343043\n",
      "test loss:  tensor(2.1289) ; test corr:  0.2773516462075406\n",
      "epoch:  19\n",
      "train loss:  tensor(1.0822) ; train corr:  0.6765151462639991\n",
      "test loss:  tensor(1.9224) ; test corr:  0.2852368895195336\n",
      "epoch:  20\n",
      "train loss:  tensor(0.9798) ; train corr:  0.7128812578574721\n",
      "test loss:  tensor(1.9493) ; test corr:  0.2839837208175719\n",
      "epoch:  21\n",
      "train loss:  tensor(0.9240) ; train corr:  0.7323725945129607\n",
      "test loss:  tensor(1.9716) ; test corr:  0.2865441313940344\n",
      "epoch:  22\n",
      "train loss:  tensor(0.8299) ; train corr:  0.7744620972304955\n",
      "test loss:  tensor(1.8830) ; test corr:  0.28864846037021\n",
      "epoch:  23\n",
      "train loss:  tensor(0.8530) ; train corr:  0.7765880312954885\n",
      "test loss:  tensor(1.7849) ; test corr:  0.28397805910608426\n",
      "epoch:  24\n",
      "train loss:  tensor(0.8468) ; train corr:  0.7686413892689163\n",
      "test loss:  tensor(1.8280) ; test corr:  0.2810740326284446\n",
      "epoch:  25\n",
      "train loss:  tensor(0.9333) ; train corr:  0.7368979680977702\n",
      "test loss:  tensor(1.9057) ; test corr:  0.25708717423305444\n",
      "epoch:  26\n",
      "train loss:  tensor(0.8852) ; train corr:  0.7584186507670969\n",
      "test loss:  tensor(1.8717) ; test corr:  0.2792414182285163\n",
      "epoch:  27\n",
      "train loss:  tensor(0.9353) ; train corr:  0.7412339081202841\n",
      "test loss:  tensor(1.8620) ; test corr:  0.2556264005106959\n",
      "epoch:  28\n",
      "train loss:  tensor(0.9170) ; train corr:  0.7466530971611972\n",
      "test loss:  tensor(1.8572) ; test corr:  0.2714935873038987\n",
      "epoch:  29\n",
      "train loss:  tensor(0.9302) ; train corr:  0.73639049442876\n",
      "test loss:  tensor(1.9552) ; test corr:  0.24635464936106988\n",
      "epoch:  30\n",
      "train loss:  tensor(0.9069) ; train corr:  0.7431842183694706\n",
      "test loss:  tensor(2.0034) ; test corr:  0.22957172392920674\n",
      "epoch:  31\n",
      "train loss:  tensor(0.9195) ; train corr:  0.7383081626914653\n",
      "test loss:  tensor(2.0408) ; test corr:  0.23442850339824653\n",
      "epoch:  32\n",
      "train loss:  tensor(0.8860) ; train corr:  0.7523925837470817\n",
      "test loss:  tensor(2.0310) ; test corr:  0.2241317923500735\n",
      "epoch:  33\n",
      "train loss:  tensor(0.9813) ; train corr:  0.7194560363022529\n",
      "test loss:  tensor(2.0027) ; test corr:  0.2650232444828117\n",
      "epoch:  34\n",
      "train loss:  tensor(0.9999) ; train corr:  0.7361563573152671\n",
      "test loss:  tensor(2.0389) ; test corr:  0.26081954465146784\n",
      "epoch:  35\n",
      "train loss:  tensor(0.8935) ; train corr:  0.7616261971660643\n",
      "test loss:  tensor(2.0010) ; test corr:  0.19883872966970215\n",
      "epoch:  36\n",
      "train loss:  tensor(0.9013) ; train corr:  0.7470675594172421\n",
      "test loss:  tensor(1.9629) ; test corr:  0.23662745168699087\n",
      "epoch:  37\n",
      "train loss:  tensor(0.8225) ; train corr:  0.7758420279557057\n",
      "test loss:  tensor(1.9665) ; test corr:  0.2348297869608693\n",
      "epoch:  38\n",
      "train loss:  tensor(0.8168) ; train corr:  0.7859530106158328\n",
      "test loss:  tensor(1.9232) ; test corr:  0.2260185160639956\n",
      "epoch:  39\n",
      "train loss:  tensor(0.8462) ; train corr:  0.7765630500282249\n",
      "test loss:  tensor(1.9436) ; test corr:  0.20079386685438108\n",
      "epoch:  40\n",
      "train loss:  tensor(0.8498) ; train corr:  0.7661357097133104\n",
      "test loss:  tensor(2.0203) ; test corr:  0.20171461958967643\n",
      "epoch:  41\n",
      "train loss:  tensor(0.8040) ; train corr:  0.7792375769029916\n",
      "test loss:  tensor(2.0970) ; test corr:  0.2035683800569301\n",
      "epoch:  42\n",
      "train loss:  tensor(0.8298) ; train corr:  0.7762084818422408\n",
      "test loss:  tensor(1.9676) ; test corr:  0.21858887222464526\n",
      "epoch:  43\n",
      "train loss:  tensor(0.9000) ; train corr:  0.7570841804342155\n",
      "test loss:  tensor(1.9339) ; test corr:  0.2165113937552948\n",
      "epoch:  44\n",
      "train loss:  tensor(0.7879) ; train corr:  0.7888915399092653\n",
      "test loss:  tensor(1.9893) ; test corr:  0.2225215746609413\n",
      "epoch:  45\n",
      "train loss:  tensor(0.7552) ; train corr:  0.8032673318285548\n",
      "test loss:  tensor(2.0209) ; test corr:  0.24185451662345547\n",
      "epoch:  46\n",
      "train loss:  tensor(0.7710) ; train corr:  0.7981841552004963\n",
      "test loss:  tensor(1.9183) ; test corr:  0.26377804816115685\n",
      "epoch:  47\n",
      "train loss:  tensor(0.7344) ; train corr:  0.807913850102484\n",
      "test loss:  tensor(1.9052) ; test corr:  0.2633420341544291\n",
      "epoch:  48\n",
      "train loss:  tensor(0.8478) ; train corr:  0.775615055279289\n",
      "test loss:  tensor(1.9687) ; test corr:  0.25082608425409925\n",
      "epoch:  49\n",
      "train loss:  tensor(0.8126) ; train corr:  0.8029542614414019\n",
      "test loss:  tensor(1.8303) ; test corr:  0.24744851061066828\n",
      "epoch:  50\n",
      "train loss:  tensor(0.8848) ; train corr:  0.7699800844007126\n",
      "test loss:  tensor(1.9064) ; test corr:  0.22303419539173208\n",
      "epoch:  51\n",
      "train loss:  tensor(0.8399) ; train corr:  0.7774039666394381\n",
      "test loss:  tensor(1.9497) ; test corr:  0.23707327358642727\n",
      "epoch:  52\n",
      "train loss:  tensor(0.9208) ; train corr:  0.7513055936420752\n",
      "test loss:  tensor(1.8553) ; test corr:  0.23867296770959096\n",
      "epoch:  53\n",
      "train loss:  tensor(0.9114) ; train corr:  0.7395952501460332\n",
      "test loss:  tensor(1.9407) ; test corr:  0.2527631801920291\n",
      "epoch:  54\n",
      "train loss:  tensor(0.9167) ; train corr:  0.7506466656293214\n",
      "test loss:  tensor(1.8698) ; test corr:  0.26103919762367545\n",
      "epoch:  55\n",
      "train loss:  tensor(0.9801) ; train corr:  0.725018267597492\n",
      "test loss:  tensor(1.8924) ; test corr:  0.24439085011691758\n",
      "epoch:  56\n",
      "train loss:  tensor(0.8722) ; train corr:  0.7620529133799833\n",
      "test loss:  tensor(1.8666) ; test corr:  0.25912814648700855\n",
      "epoch:  57\n",
      "train loss:  tensor(0.9582) ; train corr:  0.7289189695406421\n",
      "test loss:  tensor(1.9145) ; test corr:  0.2468224509947673\n",
      "epoch:  58\n",
      "train loss:  tensor(1.0341) ; train corr:  0.7109059983045055\n",
      "test loss:  tensor(1.9956) ; test corr:  0.27685458062262686\n",
      "epoch:  59\n",
      "train loss:  tensor(1.0726) ; train corr:  0.6887522538314952\n",
      "test loss:  tensor(1.9806) ; test corr:  0.276344884016786\n",
      "epoch:  0\n",
      "train loss:  tensor(1.0040) ; train corr:  0.7099726979885644\n",
      "test loss:  tensor(1.9121) ; test corr:  0.27322726560129706\n",
      "epoch:  1\n",
      "train loss:  tensor(0.9307) ; train corr:  0.7338637033075748\n",
      "test loss:  tensor(1.9315) ; test corr:  0.2819958907045933\n",
      "epoch:  2\n",
      "train loss:  tensor(0.9188) ; train corr:  0.742158989307428\n",
      "test loss:  tensor(1.8893) ; test corr:  0.27158005982448363\n",
      "epoch:  3\n",
      "train loss:  tensor(0.8627) ; train corr:  0.756975296363633\n",
      "test loss:  tensor(1.9276) ; test corr:  0.2841408977377068\n",
      "epoch:  4\n",
      "train loss:  tensor(0.8782) ; train corr:  0.7577053157312752\n",
      "test loss:  tensor(1.8853) ; test corr:  0.27148852969367754\n",
      "epoch:  5\n",
      "train loss:  tensor(0.8306) ; train corr:  0.7681999216775237\n",
      "test loss:  tensor(1.9257) ; test corr:  0.2850104893456973\n",
      "epoch:  6\n",
      "train loss:  tensor(0.8624) ; train corr:  0.7653038897749925\n",
      "test loss:  tensor(1.8728) ; test corr:  0.2715099149735679\n",
      "epoch:  7\n",
      "train loss:  tensor(0.8079) ; train corr:  0.7746523617135158\n",
      "test loss:  tensor(1.9352) ; test corr:  0.28899665487074266\n",
      "epoch:  8\n",
      "train loss:  tensor(0.8889) ; train corr:  0.7623420558851992\n",
      "test loss:  tensor(1.8496) ; test corr:  0.26444690780111657\n",
      "epoch:  9\n",
      "train loss:  tensor(0.7891) ; train corr:  0.7784908380675051\n",
      "test loss:  tensor(1.9644) ; test corr:  0.29750764632377263\n",
      "epoch:  10\n",
      "train loss:  tensor(0.9499) ; train corr:  0.7469136586297678\n",
      "test loss:  tensor(1.8356) ; test corr:  0.2521563338338483\n",
      "epoch:  11\n",
      "train loss:  tensor(0.7886) ; train corr:  0.7771899981122815\n",
      "test loss:  tensor(2.0175) ; test corr:  0.3096418577369552\n",
      "epoch:  12\n",
      "train loss:  tensor(1.0124) ; train corr:  0.7291451724201036\n",
      "test loss:  tensor(1.8234) ; test corr:  0.2448752702845362\n",
      "epoch:  13\n",
      "train loss:  tensor(0.7902) ; train corr:  0.7771627530776211\n",
      "test loss:  tensor(2.0457) ; test corr:  0.31641662094705897\n",
      "epoch:  14\n",
      "train loss:  tensor(0.9906) ; train corr:  0.7350976270844464\n",
      "test loss:  tensor(1.8310) ; test corr:  0.24738274222585435\n",
      "epoch:  15\n",
      "train loss:  tensor(0.7776) ; train corr:  0.78097874176411\n",
      "test loss:  tensor(2.0267) ; test corr:  0.31458190311854295\n",
      "epoch:  16\n",
      "train loss:  tensor(0.9736) ; train corr:  0.7378031086901047\n",
      "test loss:  tensor(1.8449) ; test corr:  0.24905364296834917\n",
      "epoch:  17\n",
      "train loss:  tensor(0.7727) ; train corr:  0.7824026331795036\n",
      "test loss:  tensor(2.0281) ; test corr:  0.3132111257170536\n",
      "epoch:  18\n",
      "train loss:  tensor(0.9830) ; train corr:  0.7334614043462145\n",
      "test loss:  tensor(1.8520) ; test corr:  0.24736011345834955\n",
      "epoch:  19\n",
      "train loss:  tensor(0.7746) ; train corr:  0.7820363338396638\n",
      "test loss:  tensor(2.0448) ; test corr:  0.3166921711255821\n",
      "epoch:  20\n",
      "train loss:  tensor(1.0033) ; train corr:  0.7247713284912158\n",
      "test loss:  tensor(1.8575) ; test corr:  0.24542530296868892\n",
      "epoch:  21\n",
      "train loss:  tensor(0.7724) ; train corr:  0.7828523676101696\n",
      "test loss:  tensor(2.0501) ; test corr:  0.31816982077224176\n",
      "epoch:  22\n",
      "train loss:  tensor(1.0200) ; train corr:  0.7186580150516314\n",
      "test loss:  tensor(1.8593) ; test corr:  0.24419621902914285\n",
      "epoch:  23\n",
      "train loss:  tensor(0.7716) ; train corr:  0.7833016019761219\n",
      "test loss:  tensor(2.0604) ; test corr:  0.3183853744400385\n",
      "epoch:  24\n",
      "train loss:  tensor(0.9985) ; train corr:  0.7242400232853164\n",
      "test loss:  tensor(1.8723) ; test corr:  0.2461325301259872\n",
      "epoch:  25\n",
      "train loss:  tensor(0.7729) ; train corr:  0.7829646229246536\n",
      "test loss:  tensor(2.0592) ; test corr:  0.3197765494544962\n",
      "epoch:  26\n",
      "train loss:  tensor(1.0189) ; train corr:  0.7159208753695235\n",
      "test loss:  tensor(1.8777) ; test corr:  0.2441840513939377\n",
      "epoch:  27\n",
      "train loss:  tensor(0.7765) ; train corr:  0.7820179429530221\n",
      "test loss:  tensor(2.0634) ; test corr:  0.3213524419260946\n",
      "epoch:  28\n",
      "train loss:  tensor(1.0156) ; train corr:  0.7161269111304444\n",
      "test loss:  tensor(1.8846) ; test corr:  0.2444553621059171\n",
      "epoch:  29\n",
      "train loss:  tensor(0.7806) ; train corr:  0.7811033872703217\n",
      "test loss:  tensor(2.0694) ; test corr:  0.32286438631850317\n",
      "apply activation: Tanh\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x_train=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/x_train_\" + str(t) + \".csv\")\n",
    "x_train=x_train.drop('Unnamed: 0', axis=1)\n",
    "y_train=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_train_\" + str(t) + \".csv\")\n",
    "y_train=y_train.drop('Unnamed: 0', axis=1)\n",
    "x_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/x_test_\" + str(t) + \".csv\")\n",
    "x_test=x_test.drop('Unnamed: 0', axis=1)\n",
    "y_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_test_\" + str(t) + \".csv\")\n",
    "y_test=y_test.drop('Unnamed: 0', axis=1)\n",
    "z_test=pd.read_csv(\"./data/model\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/z_test_\" + str(t) + \".csv\")\n",
    "z_test=z_test.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "\n",
    "\n",
    "x_train = torch.FloatTensor(x_train.to_numpy()).to(device)\n",
    "y_train = torch.FloatTensor(y_train.to_numpy()).to(device)\n",
    "x_test = torch.FloatTensor(x_test.to_numpy()).to(device)\n",
    "y_test = torch.FloatTensor(y_test.to_numpy()).to(device)\n",
    "\n",
    "mis_rec = []\n",
    "\n",
    "\n",
    "step_size=0.01\n",
    "    #print(\"cross: \", cross_validate_index)\n",
    "    # set random seed\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "    # load data\n",
    "    #x_train, y_train, x_test, y_test = preprocess_data(data_name, cross_validate_index)\n",
    "\n",
    "\n",
    "    #if data_name == 'MNIST':\n",
    "    #    x_train, x_test = x_train.view(x_train.size(0), -1), x_test.view(x_test.size(0), -1)\n",
    "    #    x_train, x_test, y_train, y_test = x_train[:20000], x_test[:20000], y_train[:20000], y_test[:20000]\n",
    "\n",
    "\n",
    "    ###### pretraining\n",
    "#start_time = time.clock()\n",
    "net, last_hidden_layer_train, last_hidden_layer_test = train_stonet(x_train, x_test, y_train, y_test,\n",
    "            PATH=base_path + data_name+'/reduce_dim_' + str(net_architecture[-2]) + '/cross_'+str(cross_validate_index)+'/', net_index=\"stonet\", net_architecture=net_architecture, num_epochs=num_epochs, \n",
    "            subn=batch_size, sigma_list=sigma_list, alpha=alpha, MH_step=MH_step, \n",
    "            proposal_lr=proposal_lr, step_size=step_size, momentum=momentum, weight_decay=weight_decay, device=device, regression_flag=regression_flag)\n",
    "#time_elapse = time.clock() - start_time\n",
    "\n",
    "\n",
    "    ###### train the stonet with a good start\n",
    "    # set loss function\n",
    "sse = nn.MSELoss(reduction='sum')\n",
    "if regression_flag:\n",
    "    loss_func = nn.MSELoss()\n",
    "    loss_func_sum = nn.MSELoss(reduction='sum')\n",
    "    train_loss_path = np.zeros(num_epochs)\n",
    "    test_loss_path = np.zeros(num_epochs)\n",
    "    train_accuracy_path = np.zeros(num_epochs)\n",
    "    test_accuracy_path = np.zeros(num_epochs)\n",
    "else:\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    loss_func_sum = nn.CrossEntropyLoss(reduction='sum')\n",
    "    train_loss_path = np.zeros(num_epochs)\n",
    "    test_loss_path = np.zeros(num_epochs)\n",
    "    train_accuracy_path = np.zeros(num_epochs)\n",
    "    test_accuracy_path = np.zeros(num_epochs)\n",
    "\n",
    "sigma_output = sigma_list[-1]\n",
    "ntrain = x_train.shape[0]\n",
    "ntest = x_test.shape[0]\n",
    "num_block = len(net_architecture)\n",
    "a = 0.6\n",
    "step_size=0.001\n",
    "\n",
    "for epoch in range(30):\n",
    "\n",
    "        # set optimizer\n",
    "    optimizer_list = []\n",
    "    for i in range(len(net_architecture)):\n",
    "        optimizer_list.append(torch.optim.SGD(net.block_list[i].parameters(), lr=step_size/(1+step_size*(epoch)**a), momentum=momentum, weight_decay=weight_decay))\n",
    "\n",
    "    hidden_list = []\n",
    "    momentum_list = []\n",
    "    with torch.no_grad():\n",
    "        hidden_list.append(net.block_list[0](x_train))\n",
    "        momentum_list.append(torch.zeros_like(hidden_list[-1]))\n",
    "        for i in range(1, num_hidden):\n",
    "            hidden_list.append(net.block_list[i](hidden_list[-1]))\n",
    "            momentum_list.append(torch.zeros_like(hidden_list[-1]))\n",
    "\n",
    "    foward_hidden = net.block_list[0](x_train).data\n",
    "\n",
    "    for i in range(hidden_list.__len__()):\n",
    "        hidden_list[i].requires_grad = True\n",
    "\n",
    "    for repeat in range(MH_step):\n",
    "        for layer_index in reversed(range(num_hidden)):\n",
    "            if hidden_list[layer_index].grad is not None:\n",
    "                hidden_list[layer_index].grad.zero_()\n",
    "\n",
    "            if layer_index == num_hidden - 1:\n",
    "                hidden_likelihood = -loss_func_sum(\n",
    "                    net.block_list[layer_index + 1](hidden_list[layer_index]),\n",
    "                    y_train) / sigma_output\n",
    "            else:\n",
    "                hidden_likelihood = -sse(net.block_list[layer_index + 1](hidden_list[layer_index]),\n",
    "                                          hidden_list[layer_index + 1]) / sigma_list[layer_index + 1]\n",
    "            if layer_index == 0:\n",
    "                hidden_likelihood = hidden_likelihood - sse(\n",
    "                    foward_hidden,\n",
    "                    hidden_list[layer_index]) / sigma_list[layer_index]\n",
    "            else:\n",
    "                hidden_likelihood = hidden_likelihood - sse(\n",
    "                    net.block_list[layer_index](hidden_list[layer_index - 1]),\n",
    "                    hidden_list[layer_index]) / sigma_list[layer_index]\n",
    "\n",
    "            hidden_likelihood.backward()\n",
    "            step_proposal_lr = proposal_lr[layer_index]/(1+proposal_lr[layer_index]*(epoch)**a)\n",
    "            gamma = alpha / step_proposal_lr\n",
    "            with torch.no_grad():\n",
    "                momentum_list[layer_index] = (1 - gamma * step_proposal_lr) * momentum_list[layer_index] + step_proposal_lr * hidden_list[\n",
    "                    layer_index].grad + torch.FloatTensor(\n",
    "                    hidden_list[layer_index].shape).to(device).normal_().mul(\n",
    "                    np.sqrt(2 * gamma * step_proposal_lr))\n",
    "                hidden_list[layer_index].data += step_proposal_lr * momentum_list[layer_index]\n",
    "\n",
    "        num_optim_step = 1\n",
    "        for layer_index in range(num_block):\n",
    "            for step in range(num_optim_step):\n",
    "                if layer_index == 0:\n",
    "                    loss = sse(net.block_list[layer_index](x_train),\n",
    "                                hidden_list[layer_index]) / ntrain\n",
    "                elif layer_index == num_block - 1:\n",
    "                    loss = loss_func_sum(net.block_list[layer_index](hidden_list[layer_index - 1]),\n",
    "                                            y_train) / ntrain\n",
    "                else:\n",
    "                    loss = sse(net.block_list[layer_index](hidden_list[layer_index - 1]),\n",
    "                                hidden_list[layer_index]) / ntrain\n",
    "                optimizer_list[layer_index].zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer_list[layer_index].step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        if regression_flag:\n",
    "            print('epoch: ', epoch)\n",
    "\n",
    "            output, last_hidden_layer_train = net(x_train)\n",
    "            train_loss = loss_func(output, y_train)\n",
    "            train_loss_path[epoch] = train_loss\n",
    "            train_corr = np.corrcoef(output.squeeze(), y_train.squeeze())[0,1]\n",
    "            train_accuracy_path[epoch] = train_corr\n",
    "            print(\"train loss: \", train_loss, \"; train corr: \", train_corr)\n",
    "\n",
    "            output, last_hidden_layer_test = net(x_test)\n",
    "            test_loss = loss_func(output, y_test)\n",
    "            test_loss_path[epoch] = test_loss\n",
    "            test_corr = np.corrcoef(output.squeeze(), y_test.squeeze())[0,1]\n",
    "            test_accuracy_path[epoch] = test_corr\n",
    "            print(\"test loss: \", test_loss, \"; test corr: \", test_corr)\n",
    "\n",
    "        else:\n",
    "            print('epoch: ', epoch)\n",
    "            output, last_hidden_layer_train = net(x_train)\n",
    "            train_loss = loss_func(output, y_train)\n",
    "            prediction = output.data.max(1)[1]\n",
    "            train_accuracy = prediction.eq(y_train.data).sum().item() / ntrain\n",
    "\n",
    "            print(\"train loss: \", train_loss, 'train accuracy: ', train_accuracy)\n",
    "\n",
    "            output, last_hidden_layer_test = net(x_test)\n",
    "            test_loss = loss_func(output, y_test)\n",
    "            prediction = output.data.max(1)[1]\n",
    "            test_accuracy = prediction.eq(y_test.data).sum().item() / ntest\n",
    "\n",
    "            print(\"test loss: \", test_loss, 'test accuracy: ', test_accuracy)\n",
    "\n",
    "    # get the output of the last hidden layer\n",
    "output, last_hidden_layer_train = net(x_train)\n",
    "output, last_hidden_layer_test = net(x_test)\n",
    "\n",
    "\n",
    "\n",
    "if activation == \"ReLU\":\n",
    "    print(\"apply activation: ReLU\")\n",
    "    m = nn.ReLU()\n",
    "elif activation == \"Tanh\":\n",
    "    print(\"apply activation: Tanh\")\n",
    "    m = nn.Tanh()\n",
    "last_hidden_layer_train = m(last_hidden_layer_train)\n",
    "last_hidden_layer_test = m(last_hidden_layer_test)\n",
    "\n",
    "\n",
    "\n",
    "t1_stop = time.time() \n",
    "t2_stop = process_time()\n",
    "\n",
    "\n",
    "#last_hidden_layer_train = last_hidden_layer_train.detach().cpu().numpy()\n",
    "last_hidden_layer_test = last_hidden_layer_test.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "last_hidden_layer_test_df=pd.DataFrame(last_hidden_layer_test)\n",
    "last_hidden_layer_test_df.to_csv(\"./results-StoNet-\" + str(args.r) + \"/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/y_suff_\" + str(t) + \".csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "time_use=[t1_stop-t1_start,t2_stop-t2_start]\n",
    "time_use_df=pd.DataFrame(time_use)\n",
    "time_use_df.to_csv(\"./results-StoNet-\" + str(args.r) + \"/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \"/time_\" + str(t) + \".csv\")\n",
    "\n",
    "\n",
    "#dcor_current=dcor.distance_correlation(last_hidden_layer_test,z_test.to_numpy(),method=\"naive\")\n",
    "#dcor_list.append(dcor_current)\n",
    "#print(model1, model2, t, dcor_current)\n",
    "#dcor_list_df=pd.DataFrame(dcor_list)\n",
    "#dcor_list_df.to_csv(\"./results-StoNet-\" + str(net_architecture[0]) + \"/result-\" + str(model1) + \"-\" + str(model2) + \"-\" + str(n) + \".csv\")\n",
    "#print(model1, model2, np.mean(dcor_list), np.std(dcor_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cdaccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77d9a81-8337-4501-8757-c22acddbf72c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01668c10-ac80-4cdb-bf86-85fe837ba226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch_conda",
   "language": "python",
   "name": "torch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
